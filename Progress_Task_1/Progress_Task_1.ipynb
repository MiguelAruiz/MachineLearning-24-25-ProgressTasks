{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Task 1: Prediction of wine quality\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This progress task has the aim to predict the quality of wine based on its physicochemical properties. The dataset used in this task is the [Wine Quality Dataset](https://archive.ics.uci.edu/dataset/186/wine+quality) from the UCI Machine Learning Repository. Credits to *P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.*\n",
    "\n",
    "The objective of this task is to select an apropiate regression and classification model and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environmental variables\n",
    "\n",
    "Download the dataset and import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ucimlrepo seaborn matplotlib scikit-learn pandas numpy pygam pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 186, 'name': 'Wine Quality', 'repository_url': 'https://archive.ics.uci.edu/dataset/186/wine+quality', 'data_url': 'https://archive.ics.uci.edu/static/public/186/data.csv', 'abstract': 'Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests (see [Cortez et al., 2009], http://www3.dsi.uminho.pt/pcortez/wine/).', 'area': 'Business', 'tasks': ['Classification', 'Regression'], 'characteristics': ['Multivariate'], 'num_instances': 4898, 'num_features': 11, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['quality'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2009, 'last_updated': 'Wed Nov 15 2023', 'dataset_doi': '10.24432/C56S3T', 'creators': ['Paulo Cortez', 'A. Cerdeira', 'F. Almeida', 'T. Matos', 'J. Reis'], 'intro_paper': {'ID': 252, 'type': 'NATIVE', 'title': 'Modeling wine preferences by data mining from physicochemical properties', 'authors': 'P. Cortez, A. Cerdeira, Fernando Almeida, Telmo Matos, J. Reis', 'venue': 'Decision Support Systems', 'year': 2009, 'journal': None, 'DOI': None, 'URL': 'https://www.semanticscholar.org/paper/Modeling-wine-preferences-by-data-mining-from-Cortez-Cerdeira/bf15a0ccc14ac1deb5cea570c870389c16be019c', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].  Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\\n\\nThese datasets can be viewed as classification or regression tasks.  The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'For more information, read [Cortez et al., 2009].\\r\\nInput variables (based on physicochemical tests):\\r\\n   1 - fixed acidity\\r\\n   2 - volatile acidity\\r\\n   3 - citric acid\\r\\n   4 - residual sugar\\r\\n   5 - chlorides\\r\\n   6 - free sulfur dioxide\\r\\n   7 - total sulfur dioxide\\r\\n   8 - density\\r\\n   9 - pH\\r\\n   10 - sulphates\\r\\n   11 - alcohol\\r\\nOutput variable (based on sensory data): \\r\\n   12 - quality (score between 0 and 10)', 'citation': None}}\n",
      "                    name     role         type demographic  \\\n",
      "0          fixed_acidity  Feature   Continuous        None   \n",
      "1       volatile_acidity  Feature   Continuous        None   \n",
      "2            citric_acid  Feature   Continuous        None   \n",
      "3         residual_sugar  Feature   Continuous        None   \n",
      "4              chlorides  Feature   Continuous        None   \n",
      "5    free_sulfur_dioxide  Feature   Continuous        None   \n",
      "6   total_sulfur_dioxide  Feature   Continuous        None   \n",
      "7                density  Feature   Continuous        None   \n",
      "8                     pH  Feature   Continuous        None   \n",
      "9              sulphates  Feature   Continuous        None   \n",
      "10               alcohol  Feature   Continuous        None   \n",
      "11               quality   Target      Integer        None   \n",
      "12                 color    Other  Categorical        None   \n",
      "\n",
      "               description units missing_values  \n",
      "0                     None  None             no  \n",
      "1                     None  None             no  \n",
      "2                     None  None             no  \n",
      "3                     None  None             no  \n",
      "4                     None  None             no  \n",
      "5                     None  None             no  \n",
      "6                     None  None             no  \n",
      "7                     None  None             no  \n",
      "8                     None  None             no  \n",
      "9                     None  None             no  \n",
      "10                    None  None             no  \n",
      "11  score between 0 and 10  None             no  \n",
      "12            red or white  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    " \n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    " \n",
    "# data (as pandas dataframes) \n",
    "X = wine_quality.data.features \n",
    "y = wine_quality.data.targets \n",
    "\n",
    "df_wine = pd.concat([X,y], axis=1)\n",
    " \n",
    "# metadata \n",
    "print(wine_quality.metadata) \n",
    "\n",
    "# get variable information \n",
    "print(wine_quality.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, a brief exploratory data analysis (EDA) will be performed on the dataset, prior to correctly pre-process it and capture the most relevant features for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of instances and the number of features\n",
    "print (\"Shape of data:\", X.shape , y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first rows of the features\n",
    "print(\"=================== Feature's First Rows ===================\\n\", X.head(3), \"\\n\")\n",
    "\n",
    "# Print the first rows of the target\n",
    "print(\"=================== Target's First Rows ===================\\n\", y.head(3), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=================== Null value count ===================\\n\",df_wine.isnull().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken a look into the dataset, here's a summary:\n",
    "\n",
    "The dataset consists of 11 continuous features, none of them with missing values. The features are: \n",
    "\n",
    "* `fixed_acidity`: with values ranging from 4.6 to 15.9.\n",
    "* `volatile_acidity`: with values ranging from 0.12 to 1.58.\n",
    "* `citric_acid`: with values ranging from 0 to 1.66.\n",
    "* `residual_sugar`: with values ranging from 0.6 to 65.8.\n",
    "* `chlorides`: with values ranging from 0.009 to 0.611.\n",
    "* `free_sulfur_dioxide`: with values ranging from 1 to 289.\n",
    "* `total_sulfur_dioxide`: with values ranging from 6 to 440.\n",
    "* `density`: with values ranging from 0.99 to 1.003.\n",
    "* `pH`: with values ranging from 2.74 to 4.01.\n",
    "* `sulphates`: with values ranging from 0.33 to 2.\n",
    "* `alcohol`: with values ranging from 8.4 to 14.9.\n",
    "\n",
    "The target variable is:\n",
    "* `quality`: is an integer variable, from 0 to 10 but in this dataset it ranges from 3 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the statistical summary of the dataset has been obtained, a pairplot will be created to visualize the relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='quality', data=df_wine)\n",
    "plt.title(\"Distribution of Wine Quality Ratings\")\n",
    "plt.xlabel(\"Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the plot, the target variable `quality` is not a balanced set. The majority of the wines have a quality of 5 or 6, with a few wines having a quality of 3 or 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for i, feature in enumerate(df_wine.columns[:-1]):\n",
    "    plt.subplot(4, 3, i + 1)\n",
    "    sns.scatterplot(x=feature, y='quality', data=df_wine)\n",
    "    plt.title(feature)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplot, a strange data distribution can be observed. All the instances seem to be grouped by a certain value of the variable `quality`. The reason for this is that the target variable is **discrete**, so **it is treated as a categorical variable**.\n",
    "\n",
    "Given that no direct relation with the target can be inferred from the pairplot, the next step is to create pairplots between every pair of features. Then, the dependencies between the features will be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplot there are some interesting observations: \n",
    "\n",
    "- Fixed acidity and density seem to have a linear relationship. \n",
    "- Density seems to have a horizontal line pattern with other features, that could represent a constant value.\n",
    "\n",
    "From there, valueable information cannot be extracted, so it is necessary to continue analyzing the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "corrmat = df_wine.corr()\n",
    "sns.heatmap(corrmat, square = True, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out from the plot, the strongest correlation can be observed between the attributes **`free_sulfur_dioxide`** and **`total_sulfur_dioxide`** (0.72). The reason for this is total sulfur dioxide includes the free sulfur dioxide, so the variable free sulfur will be removed from the dataset, as both variables represent almost the same information and this will reduce redundancy.\n",
    "\n",
    "The second strongest correlation is between **`density`** and **`alcohol`** (-0.69). This correlation is negative, due to the fact that an increase in the alcohol graduation in wine leads to a loss of water quantity. Therefore, given that alcohol is less dense than water, the density of the wine decreases.\n",
    "\n",
    "maybe test to remove density as it might be a constant value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elena\\AppData\\Local\\Temp\\ipykernel_16620\\1948943.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.drop(columns=['free_sulfur_dioxide'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_wine.drop(columns=['free_sulfur_dioxide'], inplace=True)\n",
    "X.drop(columns=['free_sulfur_dioxide'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of regression models\n",
    "\n",
    "We will put here the different model evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "Simple linear regression assumes the dependency of Y on X (or $X_1$, $X_2$, ... , $X_n$) is linear. In simple linear regression, we have a single predictor X. Mathematically, we can write this linear relationship as: $Y = \\beta_0 + \\beta_1X + \\epsilon$.\n",
    "\n",
    "Let's plot again the pairplot with all the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_wine, y_vars='quality',x_vars=df_wine.columns[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a line cannot be drawn to represent the relationship between the features and the target variable. This is because the target variable is discrete. However, let's try to fit a simple linear regression model for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "y = df_wine['quality']\n",
    "for i, feature in enumerate(df_wine.columns[:-1]):\n",
    "    plt.subplot(4, 3, i + 1)\n",
    "    linear = LinearRegression()\n",
    "    X = df_wine[[feature]]  # Reshape to 2D array\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    linear.fit(X_train, y_train)\n",
    "    y_pred = linear.predict(X_test)\n",
    "    plt.scatter(X_test, y_test)\n",
    "    plt.plot(X_test, y_pred, color='red')\n",
    "    plt.title(f\"{feature} {r2_score(y_test, y_pred)}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows, what we expected. That a simple linear regression model cannot be used to predict the quality of the wine that is a discrete variable.\n",
    "\n",
    "The highest $R^2$ score is 0.18, for the feature alcohol, which is very low. The rest are close to 0.\n",
    "\n",
    "**Conclusion**: Can't use simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilinear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilinear Regression - Ridge criterion\n",
    "\n",
    "The following block of code will make the preparations for a multilinear regression model using the Ridge criterion. The model will be trained and evaluated using the dataset. Firstly, the train-test division will be performed, then the model will be trained and evaluated following a cross validation factor of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the Train-Test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train data shape: \", X_train.shape, y_train.shape)\n",
    "\n",
    "print(\"Test data shape: \", X_test.shape, y_test.shape)\n",
    "\n",
    "# Create the Ridge Multilinear Regression model. We will\n",
    "from sklearn.linear_model import RidgeCV\n",
    "ridge_regressor = RidgeCV(cv=5)\n",
    "\n",
    "# Fit the model\n",
    "ridge_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model has been successfully trained. Now, some metrics will be extracted from it:\n",
    "As we can see, a line cannot be drawn to represent the relationship between the features and the target variable. This is because the target variable is discrete. However, let's try to fit a simple linear regression model for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best lambda (alpha) selected by cross-validation\n",
    "best_lambda = ridge_regressor.alpha_\n",
    "print(f\"Best lambda selected by RidgeCV: {best_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is tested on the test set and is evaluated by the following metrics:\n",
    "\n",
    "- Mean Squares Error (MAE)\n",
    "- R² score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training and testing sets\n",
    "y_train_pred = ridge_regressor.predict(X_train)\n",
    "y_test_pred = ridge_regressor.predict(X_test)\n",
    "\n",
    "# Calculate both Mean Squared Error and R2 Score\n",
    "train_mse_ridge = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse_ridge = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "train_r2_ridge = r2_score(y_train, y_train_pred)\n",
    "test_r2_ridge = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train MSE: {train_mse_ridge}\")\n",
    "print(f\"Test MSE: {test_mse_ridge}\")\n",
    "print(f\"Train R2: {train_r2_ridge}\")\n",
    "print(f\"Test R2: {test_r2_ridge}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The model has not a good performance, the MSE is high and the R² score is low. This means that the model is not able to accurately predict the quality of the wine based on the physicochemical properties. This may be due to the fact that the target variable is discrete and not continuous, so a classification approach may be more suitable for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilienar Regression - Lasso criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Lasso Multilinear Regression model, cross validation will be done in a similar manner as with the Ridge model from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=41)\n",
    "\n",
    "print(\"Train data shape: \", X_train.shape, y_train.shape)\n",
    "\n",
    "print(\"Test data shape: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model\n",
    "\n",
    "lasso_regressor = LassoCV(cv=5)\n",
    "lasso_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda = lasso_regressor.alpha_\n",
    "print(f\"Best lambda selected by LassoCV: {best_lambda}\")\n",
    "del best_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is tested on the test set and is evaluated by the following metrics:\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- R² score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training and testing sets\n",
    "y_train_pred = ridge_regressor.predict(X_train)\n",
    "y_test_pred = ridge_regressor.predict(X_test)\n",
    "\n",
    "# Calculate both Mean Squared Error and R2 Score\n",
    "train_mse_lasso = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse_lasso = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "train_r2_lasso = r2_score(y_train, y_train_pred)\n",
    "test_r2_lasso = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train MSE: {train_mse_lasso}\")\n",
    "print(f\"Test MSE: {test_mse_lasso}\")\n",
    "print(f\"Train R2: {train_r2_lasso}\")\n",
    "print(f\"Test R2: {test_r2_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: The model has an almost identical performance to the one with Ridge criterion. This means it also has not a very good performance, for the same reasons as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "\n",
    "The Random Forest Regressor model is based on decorrelated trees that reduce the variance of the model and reduce overfitting. This is a powerful tool as our dataset is higly imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the parameter `stratify` in the `train_test_split` function to ensure that the distribution of the target variable is the same in the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_wine.drop(columns=['quality'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_wine['quality'], test_size=0.2, random_state=42, stratify=df_wine['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use the `RandomizedSearchCV` to find the best hyperparameters as the performance is more or less equal to the `GridSearchCV` but it is 10 times faster (in this case). The hyperparameters that will be tuned are:\n",
    "* n_estimators: the number of trees in the forest.\n",
    "* max_depth: the maximum depth of the tree.\n",
    "* min_samples_split: the minimum number of samples required to split an internal node.\n",
    "* min_samples_leaf: the minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution for Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 5)\n",
    "}\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "                                n_iter=50, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "y_pred_random = random_search.predict(X_test)\n",
    "\n",
    "# Print statics\n",
    "print(\"Best Parameters from Random Search:\", random_search.best_params_)\n",
    "print(\"Best Score from Random Search:\", random_search.best_score_)\n",
    "print(\"----Test set----\")\n",
    "print(\"MSE = \", mean_squared_error(y_test, y_pred_random))\n",
    "print(\"R2 = \", r2_score(y_test, y_pred_random))\n",
    "print(\"MAE = \", mean_absolute_error(y_test, y_pred_random))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, the resulting best hyperparameters are:\n",
    "* n_estimators: 138\n",
    "* min_samples_split: 2\n",
    "* min_samples_leaf: 1\n",
    "* max_depth: 20\n",
    "\n",
    "About the **performance metrics**, the best model achieves the following results:\n",
    "* MSE = 0.387 which is lower than the variance of the target variable, so the mode's error is lower than the dispersion of the target variable.\n",
    "* $R^2$ = 0.492 means that the model explains almost half of the variance of the `quality` variable.\n",
    "* MAE = 0.441 means the error between the predicted and the true value is 0.441\n",
    "\n",
    "Most of the `quality` values are between 7 and 5, so the model is better at predicting these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance of the variable quality = \", df_wine['quality'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_random}, index=X_test.index)\n",
    "plot_df.sort_index(inplace=True)\n",
    "\n",
    "# Plotting the actual vs predicted values\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(plot_df.index, plot_df['Actual'], label='Actual', color='lightblue', marker='o', markersize=4)\n",
    "plt.plot(plot_df.index, plot_df['Predicted'], label='Predicted', color='salmon', marker='x', markersize=4)\n",
    "\n",
    "plt.title('Actual vs Predicted Values for Random Forest Regressor')\n",
    "plt.ylabel('Quality')\n",
    "plt.legend()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the importance of the features in the model to test if reducing the dimensionality of the dataset could improve the model.\n",
    "This are the importance of the features in the random forest regressor model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Feature Relevances')\n",
    "print(pd.DataFrame({'Attributes': X_train.columns,\n",
    "            'Feature importance':random_search.best_estimator_.feature_importances_}).sort_values('Feature importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment for retraining with a subset\n",
    "# new_df = df_wine[['alcohol', 'volatile_acidity', 'residual_sugar', 'total_sulfur_dioxide', 'sulphates', 'pH']]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(new_df, df_wine['quality'], test_size=0.2, random_state=42, stratify=df_wine['quality'])\n",
    "\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "# param_dist = {\n",
    "#     'n_estimators': randint(50, 300),\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': randint(2, 11),\n",
    "#     'min_samples_leaf': randint(1, 5)\n",
    "# }\n",
    "# random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "#                                 n_iter=50, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "# random_search.fit(X_train, y_train)\n",
    "# y_pred_random_subset = random_search.predict(X_test)\n",
    "\n",
    "# # Print statics\n",
    "# print(\"Best Parameters from Random Search:\", random_search.best_params_)\n",
    "# print(\"Best Score from Random Search:\", random_search.best_score_)\n",
    "# print(\"----Test set----\")\n",
    "# print(f\"MSE(subset) = {mean_squared_error(y_test, y_pred_random_subset):.3f} vs MSE(full) = {mean_squared_error(y_test, y_pred_random):.3f}\")\n",
    "# print(f\"R2(subset) = {r2_score(y_test, y_pred_random_subset):.3f} vs R2(full) = {r2_score(y_test, y_pred_random):.3f}\")\n",
    "# print(f\"MAE(subset) = {mean_absolute_error(y_test, y_pred_random_subset):.3f} vs MAE(full) = {mean_absolute_error(y_test, y_pred_random):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features is the `alcohol` whereas the least important is the `citric_acid`. Although training the model again with a subset of the features improves the time, it does not improve the performance of the model. Taking int account that the error should decrease and the $R^2$ should increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for Random Forest Regressor**: This might be a good model if we want to focus on the most common values of the quality (5, 6 and 7). However, the model is not able to predict the extreme values of the quality (3 and 9).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Additive Models (GAMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAMs are derivations of a standard linear models which apply non-linear functions to predictors, all whilst mainting the additive structure of the model. They can be taken as an abstraction for a linear regression model.\n",
    "\n",
    "Their formula can be written as: \n",
    "\n",
    "$y_i$ = $\\beta_0$ + $\\sum_{j=1}^p f_j(x_{ij}) + \\epsilon_i $\n",
    "\n",
    "As explained before, regression models are not suitable for this problem due to the discrete nature of the target variable. Nevertheless, let's try to fit a GAM model to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import LinearGAM, s, l\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the training and testing sets\n",
    "print(\"Train data shape: \", X_train.shape, y_train.shape)\n",
    "print(\"Test data shape: \", X_test.shape, y_test.shape)\n",
    "# Create the Generalized Additive Model\n",
    "gam_regressor = LinearGAM(  l(0) +\n",
    "                            s(1) +\n",
    "                            l(2) +\n",
    "                            s(3) +\n",
    "                            l(4) +\n",
    "                            s(5) +\n",
    "                            l(6) +\n",
    "                            s(7) +\n",
    "                            l(8) +\n",
    "                            s(9)\n",
    "                        ).fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to make predictions on the test set and evaluate the model using the following metrics:\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- R² score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the train and test set \n",
    "y_train_pred = gam_regressor.predict(X_train)\n",
    "y_test_pred = gam_regressor.predict(X_test)\n",
    "\n",
    "# Compare the results by using MSE and R2\n",
    "train_mse_gam = mean_squared_error(y_train, y_train_pred)\n",
    "print(f\"Train MSE: {train_mse_gam}\")\n",
    "\n",
    "test_mse_gam = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse_gam}\")\n",
    "\n",
    "train_r2_gam = r2_score(y_train, y_train_pred)\n",
    "print(f\"Train R2: {train_r2_gam}\")\n",
    "\n",
    "test_r2_gam = r2_score(y_test, y_test_pred)\n",
    "print(f\"Test R2: {test_r2_gam}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metrics do not look good at all. A graphical description of the performance is shown in the plots below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function plot\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, 10, figsize=(20, 5))\n",
    "\n",
    "titles = ['Fixed Acidity', 'Volatile Acidity', 'Citric Acid', 'Residual Sugar', 'Chlorides', 'Total Sulfur Dioxide', 'Density', 'pH', 'Sulphates', 'Alcohol']\n",
    "for i, ax in enumerate(axs):\n",
    "    XX = gam_regressor.generate_X_grid(term=i)\n",
    "    ax.plot(XX[:, i], gam_regressor.partial_dependence(term=i, X=XX))\n",
    "    ax.plot(XX[:, i], gam_regressor.partial_dependence(term=i, X=XX, width=.95)[1], c='r', ls='--')\n",
    "    if i == 0:\n",
    "        ax.set_ylim(-30,30)\n",
    "    ax.set_title(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the plots, a summary of the model's performance is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_regressor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The P-Value is way too low, which means that the predictions of the model are very unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** As expected, both MSE and R² score report very low performance, and so do the plots and the summary, which means that the model is not able to accurately predict the quality of the wine based on the physicochemical properties. This may be due to the fact that the target variable is discrete and not continuous, so a classification approach may be more suitable for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final decision\n",
    "\n",
    "Here we mention the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of classification models\n",
    "\n",
    "We will put here the different model evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes is a classiffication algorithm based on Bayes' theorem. It assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Its aim is to predict that the n-dimensional feature vector $X=(x_1,x_2,...,x_n)$ belongs to class $Y_i$, encountered within a set of classes $C=(Y_1,Y_2,...,Y_n)$ that verifies this condition:\n",
    "\n",
    "arg max $(P(Y_i|X))$.\n",
    "\n",
    "This algorithm would only work in the case of categorical features, in algorithms like **Multinomial Naïve Bayes**.\n",
    "\n",
    "In order to estimate the class-conditional probabilities for continuous features, there are well-known techniques which assume that the likelihood of the features follows a certain probability distribution. **Gaussian Naïve Bayes**, for example, assumes a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naïve Bayes assumes Gaussian distribution for the likelihood of numeric continuous features:\n",
    "\n",
    "$g(x_i,μ,\\sigma)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{(x_i-μ)^2}{2\\sigma^2}$\n",
    "\n",
    "so that the likelihood of the features is calculated as:\n",
    "\n",
    "$P(X_i = x_i|Y_i = y_i)=g(x_i,μ_{Y_i},\\sigma_{Y_i})$\n",
    "\n",
    "This algorithm is suitable for the dataset, as it contains a discrete target feature. The main issue will be to perform discretization on the target variable, which is still numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "bins = (2, 5.5, 9)\n",
    "labels= [\"poor quality\", \"nice quality\"]\n",
    "\n",
    "# Perform discretization on the continuous features\n",
    "y_discretized = pd.cut(df_wine['quality'], bins=bins, labels=labels)\n",
    "\n",
    "# Show that the target has been discretized\n",
    "y_discretized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is now categorical, so the Gaussian Naïve Bayes algorithm can be used. Its most important assumption is the normality of the data, this means, that if the features used are normally distributed, the algorithm will perform better. This normality can be checked by plotting the histogram of the features, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.var())\n",
    "df_dummy = pd.DataFrame(X, columns=X.columns)\n",
    "df_dummy['quality'] = y\n",
    "# Create a pair plot with the 'target' variable as the hue\n",
    "sns.pairplot(df_dummy, hue='num', diag_kind='kde', palette='coolwarm')\n",
    "plt.suptitle('Pair plot of wine features:', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a subset of features whose relation follows a probability distribution close to normality. Those are `fixed_acidity`,`citric_acid`, `ph` and `sulphates`. We will use this for the Gaussian Naïve Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create the Naive Bayes model and select features with normality\n",
    "gaussian_nb_classifier = GaussianNB()\n",
    "selected_features = ['fixed_acidity', 'residual_sugar', 'total_sulfur_dioxide', 'alcohol']\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "\n",
    "\n",
    "# Separate the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_discretized, test_size=0.2, random_state=42)\n",
    "\n",
    "gaussian_nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gaussian_nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained and tested, some metrics are extracted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other metrics can be extracted from this, such as a summary or a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gaussian_nb_classifier.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The rates are quite equal either for classifying the quality of the wine as good or bad, and are both above 50%. This means the model has nice performance measures, but they can be by far improved by other models. Without considering the room for improvement, the increase of performance by the use of a classification model (suitable for the proposed problem) is clearly noticeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already explained [Decision Trees](#Decision-Tree-Regression) in the regression section. In that case, we decided not to use it because the target variable is discrete.\n",
    "\n",
    "However, in the case of classification, Decision Trees are a good option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = (2, 5.5, 9)\n",
    "labels= [\"poor quality\", \"nice quality\"]\n",
    "\n",
    "# Perform discretization on the continuous features\n",
    "y_discretized = pd.cut(df_wine['quality'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y_discretized, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use GridSearchCV, not RandomizedSearchCV, to find the best hyperparameters beacuse we want to try all the possible combinations for the criterion parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 522 candidates, totalling 2610 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                       14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "                                       24, 25, 26, 27, 28, 29, None],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 4, 6, 8, 10, 20]},\n",
       "             scoring=&#x27;f1_macro&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                       14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "                                       24, 25, 26, 27, 28, 29, None],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 4, 6, 8, 10, 20]},\n",
       "             scoring=&#x27;f1_macro&#x27;, verbose=1)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: DecisionTreeClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=20, random_state=42)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=20, random_state=42)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                         'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                       14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "                                       24, 25, 26, 27, 28, 29, None],\n",
       "                         'min_samples_split': [2, 4, 6, 8, 10, 20]},\n",
       "             scoring='f1_macro', verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the possible hyperparameters \n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'], \n",
    "    'max_depth': list(range(2, 30)) + [None], \n",
    "    'min_samples_split': [2, 4, 6, 8, 10, 20],  \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dtc,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='f1_macro',  \n",
    "                           cv=5,  \n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 2}\n",
      "Best score (f1-macro): 0.7482495086724786\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best score (f1-macro): {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, the resulting best hyperparameters are:\n",
    "* criterion: gini\n",
    "* max_depth: 20, the maximum depth of the tree.\n",
    "* min_samples_split: 2, the minimum number of samples required to split an internal node.\n",
    "\n",
    "The Gini Index criterion considers a binary split for each feature and the best split, the one that maximizes the reduction in impurity is selected. The impurity of a node represents how mixed the classes are in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "nice quality       0.83      0.79      0.81       849\n",
      "poor quality       0.64      0.69      0.66       451\n",
      "\n",
      "    accuracy                           0.76      1300\n",
      "   macro avg       0.73      0.74      0.74      1300\n",
      "weighted avg       0.76      0.76      0.76      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics show that the model has a good performance in general with 76% of accuracy. Again the model is better at predicting the `nice quality` values but it can perform well with the other class as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: It might be a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier needs to be trained on categorical or discrete data, which is the case of the target variable `quality`. Therefore, the target variable will be transformed into a categorical variable with the following bins:\n",
    "* 3-5: poor quality\n",
    "* 6-9: nice quality\n",
    "\n",
    "We choose this binning because the target variable is not balanced at all. The mayority of the wines have a quality of 5 or 6, so we have tried to split it in two categories, to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins and category labels\n",
    "bins = (2, 5.5, 9)\n",
    "labels= [\"poor quality\", \"nice quality\"]\n",
    "\n",
    "y_disc = pd.cut(df_wine['quality'], bins=bins, labels=labels)\n",
    "X_disc = df_wine.drop(columns=['quality'])\n",
    "\n",
    "df_wine_discretized = pd.concat([X_disc, y_disc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='quality', data=df_wine, hue = y_disc)\n",
    "plt.title(\"Distribution of Wine Quality Ratings\")\n",
    "plt.xlabel(\"Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add text as legend\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height() +15), ha='center', va='baseline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine_discretized['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the transformation, the categories are split as follows:\n",
    "* Nice quality: 4113 instances\n",
    "* Poor quality: 2384 instances\n",
    "\n",
    "The classes are still not balanced, but the difference between qualities is not as big as before.\n",
    "\n",
    "Random Forest is robust to unbalanced classes, as it is based on majority voting. Also, to train it, we will use the parameter `class_weight='balanced'` so the weights of the classes are inversely proportional to the class frequencies. This will help to balance the data and to penalize more the errors in the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_disc, y_disc, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution for Random Search\n",
    "param_dist_random = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 5)\n",
    "}\n",
    "\n",
    "# Initialize Random Search\n",
    "# n_iter, which controls the number of different combinations to be tested\n",
    "random_search = RandomizedSearchCV(estimator=rfc, param_distributions=param_dist_random,\n",
    "                                   n_iter=100, cv=5, n_jobs=-1, verbose=2,\n",
    "                                   random_state=42, scoring='f1_weighted')\n",
    "\n",
    "# Fit the Random Search model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters from Random Search:\", random_search.best_params_)\n",
    "print(\"Best Score from Random Search:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = random_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "f1_score = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics are calculated using the parameter `average='weighted'` to take into account the class imbalance.\n",
    "The accuracy shows a value of 0.82, which means 82% of the predictions are correct. The F1 score is 0.82, which is the metric for imbalance classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow, there's the confusion matrix that shows how the model is performing. The diagonal values are the correct predictions.\n",
    "\n",
    "We can see that the model is good at predicting the 'nice quality' values, but not so good at predicting the 'poor quality' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=random_search.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Confusion Matrix for Random Forest Classifier\\nAccuracy: {accuracy:.2f}\")\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The Random Forest Classifier is a good model for this dataset. It has a good accuracy and F1 score, and it is robust to unbalanced classes. It shows a good performance on both of them, although it is better at predicting the 'nice quality' values (the `quality` values from 6 to 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K-Nearest Neighbors) \"Lazy Learner\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final decision\n",
    "\n",
    "Here we mention the best model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
