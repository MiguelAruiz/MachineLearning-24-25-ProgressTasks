{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Task 1: Prediction of wine quality\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This progress task has the aim to predict the quality of wine based on its physicochemical properties. The dataset used in this task is the [Wine Quality Dataset](https://archive.ics.uci.edu/dataset/186/wine+quality) from the UCI Machine Learning Repository. Credits to *P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.*\n",
    "\n",
    "The objective of this task is to select an apropiate regression and classification model and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environmental variables\n",
    "\n",
    "Download the dataset and import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ucimlrepo seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    " \n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    " \n",
    "# data (as pandas dataframes) \n",
    "X = wine_quality.data.features \n",
    "y = wine_quality.data.targets \n",
    "\n",
    "df_wine = pd.concat([X,y], axis=1)\n",
    " \n",
    "# metadata \n",
    "print(wine_quality.metadata) \n",
    "\n",
    "# get variable information \n",
    "print(wine_quality.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, a brief exploratory data analysis (EDA) will be performed on the dataset, prior to correctly pre-process it and capture the most relevant features for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of instances and the number of features\n",
    "print (\"Shape of data:\", X.shape , y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first rows of the features\n",
    "print(\"=================== Feature's First Rows ===================\\n\", X.head(3), \"\\n\")\n",
    "\n",
    "# Print the first rows of the target\n",
    "print(\"=================== Target's First Rows ===================\\n\", y.head(3), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=================== Null value count ===================\\n\",df_wine.isnull().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken a look into the dataset, here's a summary:\n",
    "\n",
    "The dataset consists of 11 continuous features, none of them with missing values. The features are: \n",
    "\n",
    "* `fixed_acidity`: with values ranging from 4.6 to 15.9.\n",
    "* `volatile_acidity`: with values ranging from 0.12 to 1.58.\n",
    "* `citric_acid`: with values ranging from 0 to 1.66.\n",
    "* `residual_sugar`: with values ranging from 0.6 to 65.8.\n",
    "* `chlorides`: with values ranging from 0.009 to 0.611.\n",
    "* `free_sulfur_dioxide`: with values ranging from 1 to 289.\n",
    "* `total_sulfur_dioxide`: with values ranging from 6 to 440.\n",
    "* `density`: with values ranging from 0.99 to 1.003.\n",
    "* `pH`: with values ranging from 2.74 to 4.01.\n",
    "* `sulphates`: with values ranging from 0.33 to 2.\n",
    "* `alcohol`: with values ranging from 8.4 to 14.9.\n",
    "\n",
    "The target variable is:\n",
    "* `quality`: is an integer variable, from 0 to 10 but in this dataset it ranges from 3 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the statistical summary of the dataset has been obtained, a pairplot will be created to visualize the relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='quality', data=df_wine)\n",
    "plt.title(\"Distribution of Wine Quality Ratings\")\n",
    "plt.xlabel(\"Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the plot, the target variable `quality` is not a balanced set. The majority of the wines have a quality of 5 or 6, with a few wines having a quality of 3 or 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_wine, y_vars='quality',x_vars=df_wine.columns[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplot, a strange data distribution can be observed. All the instances seem to be grouped by a certain value of the variable `quality`. The reason for this is that the target variable is **discrete**, so **it is treated as a categorical variable**.\n",
    "\n",
    "Given that no direct relation with the target can be inferred from the pairplot, the next step is to create pairplots between every pair of features. Then, the dependencies between the features will be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplot there are some interesting observations: \n",
    "\n",
    "- Fixed acidity and density seem to have a linear relationship. \n",
    "- Density seems to have a horizontal line pattern with other features, that could represent a constant value.\n",
    "\n",
    "From there, valueable information cannot be extracted, so it is necessary to continue analyzing the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corrmat = X.corr()\n",
    "sns.heatmap(corrmat, square = True, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out from the plot, the strongest correlation can be observed between the attributes **`free_sulfur_dioxide`** and **`total_sulfur_dioxide`** (0.72). The reason for this is total sulfur dioxide includes the free sulfur dioxide, so the variable free sulfur will be removed from the dataset, as both variables represent almost the same information and this will reduce redundancy.\n",
    "\n",
    "The second strongest correlation is between **`density`** and **`alcohol`** (-0.69). This correlation is negative, due to the fact that an increase in the alcohol graduation in wine leads to a loss of water quantity. Therefore, given that alcohol is less dense than water, the density of the wine decreases.\n",
    "\n",
    "maybe test to remove density as it might be a constant value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.drop(columns=['free_sulfur_dioxide'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of regression models\n",
    "\n",
    "We will put here the different model evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "\n",
    "The Random Forest Regressor model is based on decorrelated trees that reduce the variance of the model and reduce overfitting. This is a powerful tool as our dataset is higly imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the parameter `stratify` in the `train_test_split` function to ensure that the distribution of the target variable is the same in the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_wine.drop(columns=['quality'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_wine['quality'], test_size=0.2, random_state=42, stratify=df_wine['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use the `RandomizedSearchCV` to find the best hyperparameters as the performance is more or less equal to the `GridSearchCV` but it is 10 times faster (in this case). The hyperparameters that will be tuned are:\n",
    "* n_estimators: the number of trees in the forest.\n",
    "* max_depth: the maximum depth of the tree.\n",
    "* min_samples_split: the minimum number of samples required to split an internal node.\n",
    "* min_samples_leaf: the minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution for Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 5)\n",
    "}\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "                                n_iter=50, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "y_pred_random = random_search.predict(X_test)\n",
    "\n",
    "# Print statics\n",
    "print(\"Best Parameters from Random Search:\", random_search.best_params_)\n",
    "print(\"Best Score from Random Search:\", random_search.best_score_)\n",
    "print(\"----Test set----\")\n",
    "print(\"MSE = \", mean_squared_error(y_test, y_pred_random))\n",
    "print(\"R2 = \", r2_score(y_test, y_pred_random))\n",
    "print(\"MAE = \", mean_absolute_error(y_test, y_pred_random))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, the resulting best hyperparameters are:\n",
    "* n_estimators: 138\n",
    "* min_samples_split: 2\n",
    "* min_samples_leaf: 1\n",
    "* max_depth: 20\n",
    "\n",
    "About the **performance metrics**, the best model achieves the following results:\n",
    "* MSE = 0.387 which is lower than the variance of the target variable, so the mode's error is lower than the dispersion of the target variable.\n",
    "* $R^2$ = 0.492 means that the model explains almost half of the variance of the `quality` variable.\n",
    "* MAE = 0.441 means the error between the predicted and the true value is 0.441\n",
    "\n",
    "Most of the `quality` values are between 7 and 5, so the model is better at predicting these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance of the variable quality = \", df_wine['quality'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_random}, index=X_test.index)\n",
    "plot_df.sort_index(inplace=True)\n",
    "\n",
    "# Plotting the actual vs predicted values\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(plot_df.index, plot_df['Actual'], label='Actual', color='lightblue', marker='o', markersize=4)\n",
    "plt.plot(plot_df.index, plot_df['Predicted'], label='Predicted', color='salmon', marker='x', markersize=4)\n",
    "\n",
    "plt.title('Actual vs Predicted Values for Random Forest Regressor')\n",
    "plt.ylabel('Quality')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the importance of the features in the model to test if reducing the dimensionality of the dataset could improve the model.\n",
    "This are the importance of the features in the random forest regressor model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Feature Relevances')\n",
    "print(pd.DataFrame({'Attributes': X_train.columns,\n",
    "            'Feature importance':random_search.best_estimator_.feature_importances_}).sort_values('Feature importance', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features is the `alcohol` whereas the least important is the `citric_acid`. Although training the model again with a subset of the features improves the time, it does not improve the performance of the model. Taking int account that the error should decrease and the $R^2$ should increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment for retraining with a subset\n",
    "# new_df = df_wine[['alcohol', 'volatile_acidity', 'residual_sugar', 'total_sulfur_dioxide', 'sulphates', 'pH']]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(new_df, df_wine['quality'], test_size=0.2, random_state=42, stratify=df_wine['quality'])\n",
    "\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "# param_dist = {\n",
    "#     'n_estimators': randint(50, 300),\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': randint(2, 11),\n",
    "#     'min_samples_leaf': randint(1, 5)\n",
    "# }\n",
    "# random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "#                                 n_iter=50, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "# random_search.fit(X_train, y_train)\n",
    "# y_pred_random_subset = random_search.predict(X_test)\n",
    "\n",
    "# # Print statics\n",
    "# print(\"Best Parameters from Random Search:\", random_search.best_params_)\n",
    "# print(\"Best Score from Random Search:\", random_search.best_score_)\n",
    "# print(\"----Test set----\")\n",
    "# print(f\"MSE(subset) = {mean_squared_error(y_test, y_pred_random_subset):.3f} vs MSE(full) = {mean_squared_error(y_test, y_pred_random):.3f}\")\n",
    "# print(f\"R2(subset) = {r2_score(y_test, y_pred_random_subset):.3f} vs R2(full) = {r2_score(y_test, y_pred_random):.3f}\")\n",
    "# print(f\"MAE(subset) = {mean_absolute_error(y_test, y_pred_random_subset):.3f} vs MAE(full) = {mean_absolute_error(y_test, y_pred_random):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for Random Forest Regressor**: This might be a good model if we want to focus on the most common values of the quality (5, 6 and 7). However, the model is not able to predict the extreme values of the quality (3 and 9).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final decision\n",
    "\n",
    "Here we mention the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of classification models\n",
    "\n",
    "We will put here the different model evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final decision\n",
    "\n",
    "Here we mention the best model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
