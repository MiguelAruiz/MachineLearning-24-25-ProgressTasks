{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Task 1: Prediction of wine quality\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This progress task has the aim to predict the quality of wine based on its physicochemical properties. The dataset used in this task is the [Wine Quality Dataset](https://archive.ics.uci.edu/dataset/186/wine+quality) from the UCI Machine Learning Repository. Credits to *P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.*\n",
    "\n",
    "The objective of this task is to select an apropiate regression and classification model and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environmental variables\n",
    "\n",
    "Download the dataset and import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ucimlrepo seaborn matplotlib scikit-learn pandas numpy pygam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 186, 'name': 'Wine Quality', 'repository_url': 'https://archive.ics.uci.edu/dataset/186/wine+quality', 'data_url': 'https://archive.ics.uci.edu/static/public/186/data.csv', 'abstract': 'Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests (see [Cortez et al., 2009], http://www3.dsi.uminho.pt/pcortez/wine/).', 'area': 'Business', 'tasks': ['Classification', 'Regression'], 'characteristics': ['Multivariate'], 'num_instances': 4898, 'num_features': 11, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['quality'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2009, 'last_updated': 'Wed Nov 15 2023', 'dataset_doi': '10.24432/C56S3T', 'creators': ['Paulo Cortez', 'A. Cerdeira', 'F. Almeida', 'T. Matos', 'J. Reis'], 'intro_paper': {'ID': 252, 'type': 'NATIVE', 'title': 'Modeling wine preferences by data mining from physicochemical properties', 'authors': 'P. Cortez, A. Cerdeira, Fernando Almeida, Telmo Matos, J. Reis', 'venue': 'Decision Support Systems', 'year': 2009, 'journal': None, 'DOI': None, 'URL': 'https://www.semanticscholar.org/paper/Modeling-wine-preferences-by-data-mining-from-Cortez-Cerdeira/bf15a0ccc14ac1deb5cea570c870389c16be019c', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].  Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\\n\\nThese datasets can be viewed as classification or regression tasks.  The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'For more information, read [Cortez et al., 2009].\\r\\nInput variables (based on physicochemical tests):\\r\\n   1 - fixed acidity\\r\\n   2 - volatile acidity\\r\\n   3 - citric acid\\r\\n   4 - residual sugar\\r\\n   5 - chlorides\\r\\n   6 - free sulfur dioxide\\r\\n   7 - total sulfur dioxide\\r\\n   8 - density\\r\\n   9 - pH\\r\\n   10 - sulphates\\r\\n   11 - alcohol\\r\\nOutput variable (based on sensory data): \\r\\n   12 - quality (score between 0 and 10)', 'citation': None}}\n",
      "                    name     role         type demographic  \\\n",
      "0          fixed_acidity  Feature   Continuous        None   \n",
      "1       volatile_acidity  Feature   Continuous        None   \n",
      "2            citric_acid  Feature   Continuous        None   \n",
      "3         residual_sugar  Feature   Continuous        None   \n",
      "4              chlorides  Feature   Continuous        None   \n",
      "5    free_sulfur_dioxide  Feature   Continuous        None   \n",
      "6   total_sulfur_dioxide  Feature   Continuous        None   \n",
      "7                density  Feature   Continuous        None   \n",
      "8                     pH  Feature   Continuous        None   \n",
      "9              sulphates  Feature   Continuous        None   \n",
      "10               alcohol  Feature   Continuous        None   \n",
      "11               quality   Target      Integer        None   \n",
      "12                 color    Other  Categorical        None   \n",
      "\n",
      "               description units missing_values  \n",
      "0                     None  None             no  \n",
      "1                     None  None             no  \n",
      "2                     None  None             no  \n",
      "3                     None  None             no  \n",
      "4                     None  None             no  \n",
      "5                     None  None             no  \n",
      "6                     None  None             no  \n",
      "7                     None  None             no  \n",
      "8                     None  None             no  \n",
      "9                     None  None             no  \n",
      "10                    None  None             no  \n",
      "11  score between 0 and 10  None             no  \n",
      "12            red or white  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    " \n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    " \n",
    "# data (as pandas dataframes) \n",
    "X = wine_quality.data.features \n",
    "y = wine_quality.data.targets \n",
    "\n",
    "df_wine = pd.concat([X,y], axis=1)\n",
    " \n",
    "# metadata \n",
    "print(wine_quality.metadata) \n",
    "\n",
    "# get variable information \n",
    "print(wine_quality.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, a brief exploratory data analysis (EDA) will be performed on the dataset, prior to correctly pre-process it and capture the most relevant features for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of instances and the number of features\n",
    "print (\"Shape of data:\", X.shape , y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first rows of the features\n",
    "print(\"=================== Feature's First Rows ===================\\n\", X.head(3), \"\\n\")\n",
    "\n",
    "# Print the first rows of the target\n",
    "print(\"=================== Target's First Rows ===================\\n\", y.head(3), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=================== Null value count ===================\\n\",df_wine.isnull().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken a look into the dataset, here's a summary:\n",
    "\n",
    "The dataset consists of 11 continuous features, none of them with missing values. The features are: \n",
    "\n",
    "* `fixed_acidity`: with values ranging from 4.6 to 15.9.\n",
    "* `volatile_acidity`: with values ranging from 0.12 to 1.58.\n",
    "* `citric_acid`: with values ranging from 0 to 1.66.\n",
    "* `residual_sugar`: with values ranging from 0.6 to 65.8.\n",
    "* `chlorides`: with values ranging from 0.009 to 0.611.\n",
    "* `free_sulfur_dioxide`: with values ranging from 1 to 289.\n",
    "* `total_sulfur_dioxide`: with values ranging from 6 to 440.\n",
    "* `density`: with values ranging from 0.99 to 1.003.\n",
    "* `pH`: with values ranging from 2.74 to 4.01.\n",
    "* `sulphates`: with values ranging from 0.33 to 2.\n",
    "* `alcohol`: with values ranging from 8.4 to 14.9.\n",
    "\n",
    "The target variable is:\n",
    "* `quality`: is an integer variable, from 0 to 10 but in this dataset it ranges from 3 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the statistical summary of the dataset has been obtained, a pairplot will be created to visualize the relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='quality', data=df_wine)\n",
    "plt.title(\"Distribution of Wine Quality Ratings\")\n",
    "plt.xlabel(\"Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the plot, the target variable `quality` is not a balanced set. The majority of the wines have a quality of 5 or 6, with a few wines having a quality of 3 or 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for i, feature in enumerate(df_wine.columns[:-1]):\n",
    "    plt.subplot(4, 3, i + 1)\n",
    "    sns.scatterplot(x=feature, y='quality', data=df_wine)\n",
    "    plt.title(feature)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplot, a strange data distribution can be observed. All the instances seem to be grouped by a certain value of the variable `quality`. The reason for this is that the target variable is **discrete**, so **it is treated as a categorical variable**.\n",
    "\n",
    "Given that no direct relation with the target can be inferred from the pairplot, the next step is to create pairplots between every pair of features. Then, the dependencies between the features will be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplot there are some interesting observations: \n",
    "\n",
    "- Fixed acidity and density seem to have a linear relationship. \n",
    "- Density seems to have a horizontal line pattern with other features, that could represent a constant value.\n",
    "\n",
    "From there, valueable information cannot be extracted, so it is necessary to continue analyzing the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "corrmat = df_wine.corr()\n",
    "sns.heatmap(corrmat, square = True, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out from the plot, the strongest correlation can be observed between the attributes **`free_sulfur_dioxide`** and **`total_sulfur_dioxide`** (0.72). The reason for this is total sulfur dioxide includes the free sulfur dioxide, so the variable free sulfur will be removed from the dataset, as both variables represent almost the same information and this will reduce redundancy.\n",
    "\n",
    "The second strongest correlation is between **`density`** and **`alcohol`** (-0.69). This correlation is negative, due to the fact that an increase in the alcohol graduation in wine leads to a loss of water quantity. Therefore, given that alcohol is less dense than water, the density of the wine decreases.\n",
    "\n",
    "maybe test to remove density as it might be a constant value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elena\\AppData\\Local\\Temp\\ipykernel_42412\\1948943.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.drop(columns=['free_sulfur_dioxide'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_wine.drop(columns=['free_sulfur_dioxide'], inplace=True)\n",
    "X.drop(columns=['free_sulfur_dioxide'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of regression models\n",
    "\n",
    "We will put here the different model evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "Simple linear regression assumes the dependency of Y on X (or $X_1$, $X_2$, ... , $X_n$) is linear. In simple linear regression, we have a single predictor X. Mathematically, we can write this linear relationship as: $Y = \\beta_0 + \\beta_1X + \\epsilon$.\n",
    "\n",
    "Let's plot again the pairplot with all the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_wine, y_vars='quality',x_vars=df_wine.columns[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a line cannot be drawn to represent the relationship between the features and the target variable. This is because the target variable is discrete. However, let's try to fit a simple linear regression model for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "y = df_wine['quality']\n",
    "for i, feature in enumerate(df_wine.columns[:-1]):\n",
    "    plt.subplot(4, 3, i + 1)\n",
    "    linear = LinearRegression()\n",
    "    X = df_wine[[feature]]  # Reshape to 2D array\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    linear.fit(X_train, y_train)\n",
    "    y_pred = linear.predict(X_test)\n",
    "    plt.scatter(X_test, y_test)\n",
    "    plt.plot(X_test, y_pred, color='red')\n",
    "    plt.title(f\"{feature} {r2_score(y_test, y_pred)}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows, what we expected. That a simple linear regression model cannot be used to predict the quality of the wine that is a discrete variable.\n",
    "\n",
    "The highest $R^2$ score is 0.18, for the feature alcohol, which is very low. The rest are close to 0.\n",
    "\n",
    "**Conclusion**: Can't use simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilinear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilinear Regression - Ridge criterion\n",
    "\n",
    "The following block of code will make the preparations for a multilinear regression model using the Ridge criterion. The model will be trained and evaluated using the dataset. Firstly, the train-test division will be performed, then the model will be trained and evaluated following a cross validation factor of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the Train-Test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train data shape: \", X_train.shape, y_train.shape)\n",
    "\n",
    "print(\"Test data shape: \", X_test.shape, y_test.shape)\n",
    "\n",
    "# Create the Ridge Multilinear Regression model. We will\n",
    "from sklearn.linear_model import RidgeCV\n",
    "ridge_regressor = RidgeCV(cv=5)\n",
    "\n",
    "# Fit the model\n",
    "ridge_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model has been successfully trained. Now, some metrics will be extracted from it:\n",
    "As we can see, a line cannot be drawn to represent the relationship between the features and the target variable. This is because the target variable is discrete. However, let's try to fit a simple linear regression model for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best lambda (alpha) selected by cross-validation\n",
    "best_lambda = ridge_regressor.alpha_\n",
    "print(f\"Best lambda selected by RidgeCV: {best_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is tested on the test set and is evaluated by the following metrics:\n",
    "\n",
    "- Mean Squares Error (MAE)\n",
    "- R² score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training and testing sets\n",
    "y_train_pred = ridge_regressor.predict(X_train)\n",
    "y_test_pred = ridge_regressor.predict(X_test)\n",
    "\n",
    "# Calculate both Mean Squared Error and R2 Score\n",
    "train_mse_ridge = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse_ridge = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "train_r2_ridge = r2_score(y_train, y_train_pred)\n",
    "test_r2_ridge = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train MSE: {train_mse_ridge}\")\n",
    "print(f\"Test MSE: {test_mse_ridge}\")\n",
    "print(f\"Train R2: {train_r2_ridge}\")\n",
    "print(f\"Test R2: {test_r2_ridge}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The model has not a good performance, the MSE is high and the R² score is low. This means that the model is not able to accurately predict the quality of the wine based on the physicochemical properties. This may be due to the fact that the target variable is discrete and not continuous, so a classification approach may be more suitable for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilienar Regression - Lasso criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Lasso Multilinear Regression model, cross validation will be done in a similar manner as with the Ridge model from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=41)\n",
    "\n",
    "print(\"Train data shape: \", X_train.shape, y_train.shape)\n",
    "\n",
    "print(\"Test data shape: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model\n",
    "\n",
    "lasso_regressor = LassoCV(cv=5)\n",
    "lasso_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda = lasso_regressor.alpha_\n",
    "print(f\"Best lambda selected by LassoCV: {best_lambda}\")\n",
    "del best_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is tested on the test set and is evaluated by the following metrics:\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- R² score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training and testing sets\n",
    "y_train_pred = ridge_regressor.predict(X_train)\n",
    "y_test_pred = ridge_regressor.predict(X_test)\n",
    "\n",
    "# Calculate both Mean Squared Error and R2 Score\n",
    "train_mse_lasso = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse_lasso = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "train_r2_lasso = r2_score(y_train, y_train_pred)\n",
    "test_r2_lasso = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train MSE: {train_mse_lasso}\")\n",
    "print(f\"Test MSE: {test_mse_lasso}\")\n",
    "print(f\"Train R2: {train_r2_lasso}\")\n",
    "print(f\"Test R2: {test_r2_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: The model has an almost identical performance to the one with Ridge criterion. This means it also has not a very good performance, for the same reasons as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "\n",
    "The Random Forest Regressor model is based on decorrelated trees that reduce the variance of the model and reduce overfitting. This is a powerful tool as our dataset is higly imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the parameter `stratify` in the `train_test_split` function to ensure that the distribution of the target variable is the same in the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_wine.drop(columns=['quality'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_wine['quality'], test_size=0.2, random_state=42, stratify=df_wine['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use the `RandomizedSearchCV` to find the best hyperparameters as the performance is more or less equal to the `GridSearchCV` but it is 10 times faster (in this case). The hyperparameters that will be tuned are:\n",
    "* n_estimators: the number of trees in the forest.\n",
    "* max_depth: the maximum depth of the tree.\n",
    "* min_samples_split: the minimum number of samples required to split an internal node.\n",
    "* min_samples_leaf: the minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution for Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 5)\n",
    "}\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "                                n_iter=50, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "y_pred_random = random_search.predict(X_test)\n",
    "\n",
    "# Print statics\n",
    "print(\"Best Parameters from Random Search:\", random_search.best_params_)\n",
    "print(\"Best Score from Random Search:\", random_search.best_score_)\n",
    "print(\"----Test set----\")\n",
    "print(\"MSE = \", mean_squared_error(y_test, y_pred_random))\n",
    "print(\"R2 = \", r2_score(y_test, y_pred_random))\n",
    "print(\"MAE = \", mean_absolute_error(y_test, y_pred_random))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, the resulting best hyperparameters are:\n",
    "* n_estimators: 138\n",
    "* min_samples_split: 2\n",
    "* min_samples_leaf: 1\n",
    "* max_depth: 20\n",
    "\n",
    "About the **performance metrics**, the best model achieves the following results:\n",
    "* MSE = 0.387 which is lower than the variance of the target variable, so the mode's error is lower than the dispersion of the target variable.\n",
    "* $R^2$ = 0.492 means that the model explains almost half of the variance of the `quality` variable.\n",
    "* MAE = 0.441 means the error between the predicted and the true value is 0.441\n",
    "\n",
    "Most of the `quality` values are between 7 and 5, so the model is better at predicting these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance of the variable quality = \", df_wine['quality'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_random}, index=X_test.index)\n",
    "plot_df.sort_index(inplace=True)\n",
    "\n",
    "# Plotting the actual vs predicted values\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(plot_df.index, plot_df['Actual'], label='Actual', color='lightblue', marker='o', markersize=4)\n",
    "plt.plot(plot_df.index, plot_df['Predicted'], label='Predicted', color='salmon', marker='x', markersize=4)\n",
    "\n",
    "plt.title('Actual vs Predicted Values for Random Forest Regressor')\n",
    "plt.ylabel('Quality')\n",
    "plt.legend()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the importance of the features in the model to test if reducing the dimensionality of the dataset could improve the model.\n",
    "This are the importance of the features in the random forest regressor model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Feature Relevances')\n",
    "print(pd.DataFrame({'Attributes': X_train.columns,\n",
    "            'Feature importance':random_search.best_estimator_.feature_importances_}).sort_values('Feature importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment for retraining with a subset\n",
    "# new_df = df_wine[['alcohol', 'volatile_acidity', 'residual_sugar', 'total_sulfur_dioxide', 'sulphates', 'pH']]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(new_df, df_wine['quality'], test_size=0.2, random_state=42, stratify=df_wine['quality'])\n",
    "\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "# param_dist = {\n",
    "#     'n_estimators': randint(50, 300),\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': randint(2, 11),\n",
    "#     'min_samples_leaf': randint(1, 5)\n",
    "# }\n",
    "# random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "#                                 n_iter=50, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "# random_search.fit(X_train, y_train)\n",
    "# y_pred_random_subset = random_search.predict(X_test)\n",
    "\n",
    "# # Print statics\n",
    "# print(\"Best Parameters from Random Search:\", random_search.best_params_)\n",
    "# print(\"Best Score from Random Search:\", random_search.best_score_)\n",
    "# print(\"----Test set----\")\n",
    "# print(f\"MSE(subset) = {mean_squared_error(y_test, y_pred_random_subset):.3f} vs MSE(full) = {mean_squared_error(y_test, y_pred_random):.3f}\")\n",
    "# print(f\"R2(subset) = {r2_score(y_test, y_pred_random_subset):.3f} vs R2(full) = {r2_score(y_test, y_pred_random):.3f}\")\n",
    "# print(f\"MAE(subset) = {mean_absolute_error(y_test, y_pred_random_subset):.3f} vs MAE(full) = {mean_absolute_error(y_test, y_pred_random):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features is the `alcohol` whereas the least important is the `citric_acid`. Although training the model again with a subset of the features improves the time, it does not improve the performance of the model. Taking int account that the error should decrease and the $R^2$ should increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for Random Forest Regressor**: This might be a good model if we want to focus on the most common values of the quality (5, 6 and 7). However, the model is not able to predict the extreme values of the quality (3 and 9).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Additive Models (GAMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAMs are derivations of a standard linear models which apply non-linear functions to predictors, all whilst mainting the additive structure of the model. They can be taken as an abstraction for a linear regression model.\n",
    "\n",
    "Their formula can be written as: \n",
    "\n",
    "$y_i$ = $\\beta_0$ + $\\sum_{j=1}^p f_j(x_{ij}) + \\epsilon_i $\n",
    "\n",
    "As explained before, regression models are not suitable for this problem due to the discrete nature of the target variable. Nevertheless, let's try to fit a GAM model to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import LinearGAM, s, l\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the training and testing sets\n",
    "print(\"Train data shape: \", X_train.shape, y_train.shape)\n",
    "print(\"Test data shape: \", X_test.shape, y_test.shape)\n",
    "# Create the Generalized Additive Model\n",
    "gam_regressor = LinearGAM(  l(0) +\n",
    "                            s(1) +\n",
    "                            l(2) +\n",
    "                            s(3) +\n",
    "                            l(4) +\n",
    "                            s(5) +\n",
    "                            l(6) +\n",
    "                            s(7) +\n",
    "                            l(8) +\n",
    "                            s(9)\n",
    "                        ).fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to make predictions on the test set and evaluate the model using the following metrics:\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- R² score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the train and test set \n",
    "y_train_pred = gam_regressor.predict(X_train)\n",
    "y_test_pred = gam_regressor.predict(X_test)\n",
    "\n",
    "# Compare the results by using MSE and R2\n",
    "train_mse_gam = mean_squared_error(y_train, y_train_pred)\n",
    "print(f\"Train MSE: {train_mse_gam}\")\n",
    "\n",
    "test_mse_gam = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse_gam}\")\n",
    "\n",
    "train_r2_gam = r2_score(y_train, y_train_pred)\n",
    "print(f\"Train R2: {train_r2_gam}\")\n",
    "\n",
    "test_r2_gam = r2_score(y_test, y_test_pred)\n",
    "print(f\"Test R2: {test_r2_gam}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metrics do not look good at all. A graphical description of the performance is shown in the plots below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function plot\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, 10, figsize=(20, 5))\n",
    "\n",
    "titles = ['Fixed Acidity', 'Volatile Acidity', 'Citric Acid', 'Residual Sugar', 'Chlorides', 'Total Sulfur Dioxide', 'Density', 'pH', 'Sulphates', 'Alcohol']\n",
    "for i, ax in enumerate(axs):\n",
    "    XX = gam_regressor.generate_X_grid(term=i)\n",
    "    ax.plot(XX[:, i], gam_regressor.partial_dependence(term=i, X=XX))\n",
    "    ax.plot(XX[:, i], gam_regressor.partial_dependence(term=i, X=XX, width=.95)[1], c='r', ls='--')\n",
    "    if i == 0:\n",
    "        ax.set_ylim(-30,30)\n",
    "    ax.set_title(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the plots, a summary of the model's performance is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_regressor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The P-Value is way too low, which means that the predictions of the model are very unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** As expected, both MSE and R² score report very low performance, and so do the plots and the summary, which means that the model is not able to accurately predict the quality of the wine based on the physicochemical properties. This may be due to the fact that the target variable is discrete and not continuous, so a classification approach may be more suitable for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final decision\n",
    "\n",
    "Here we mention the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of classification models\n",
    "\n",
    "We will put here the different model evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes is a classiffication algorithm based on Bayes' theorem. It assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Its aim is to predict that the n-dimensional feature vector $X=(x_1,x_2,...,x_n)$ belongs to class $Y_i$, encountered within a set of classes $C=(Y_1,Y_2,...,Y_n)$ that verifies this condition:\n",
    "\n",
    "arg max $(P(Y_i|X))$.\n",
    "\n",
    "This algorithm would only work in the case of categorical features, in algorithms like **Multinomial Naïve Bayes**.\n",
    "\n",
    "In order to estimate the class-conditional probabilities for continuous features, there are well-known techniques which assume that the likelihood of the features follows a certain probability distribution. **Gaussian Naïve Bayes**, for example, assumes a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naïve Bayes assumes Gaussian distribution for the likelihood of numeric continuous features:\n",
    "\n",
    "$g(x_i,μ,\\sigma)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{(x_i-μ)^2}{2\\sigma^2}$\n",
    "\n",
    "so that the likelihood of the features is calculated as:\n",
    "\n",
    "$P(X_i = x_i|Y_i = y_i)=g(x_i,μ_{Y_i},\\sigma_{Y_i})$\n",
    "\n",
    "This algorithm is suitable for the dataset, as it contains a discrete target feature. The main issue will be to perform discretization on the target variable, which is still numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "bins = (2, 5.5, 9)\n",
    "labels= [\"poor quality\", \"nice quality\"]\n",
    "\n",
    "# Perform discretization on the continuous features\n",
    "y_discretized = pd.cut(df_wine['quality'], bins=bins, labels=labels)\n",
    "\n",
    "# Show that the target has been discretized\n",
    "y_discretized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is now categorical, so the Gaussian Naïve Bayes algorithm can be used. Its most important assumption is the normality of the data, this means, that if the features used are normally distributed, the algorithm will perform better. This normality can be checked by plotting the histogram of the features, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.var())\n",
    "df_dummy = pd.DataFrame(X, columns=X.columns)\n",
    "df_dummy['quality'] = y\n",
    "# Create a pair plot with the 'target' variable as the hue\n",
    "sns.pairplot(df_dummy, hue='num', diag_kind='kde', palette='coolwarm')\n",
    "plt.suptitle('Pair plot of wine features:', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a subset of features whose relation follows a probability distribution close to normality. Those are `fixed_acidity`,`citric_acid`, `ph` and `sulphates`. We will use this for the Gaussian Naïve Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create the Naive Bayes model and select features with normality\n",
    "gaussian_nb_classifier = GaussianNB()\n",
    "selected_features = ['fixed_acidity', 'residual_sugar', 'total_sulfur_dioxide', 'alcohol']\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "\n",
    "\n",
    "# Separate the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_discretized, test_size=0.2, random_state=42)\n",
    "\n",
    "gaussian_nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gaussian_nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained and tested, some metrics are extracted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other metrics can be extracted from this, such as a summary or a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gaussian_nb_classifier.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The rates are quite equal either for classifying the quality of the wine as good or bad, and are both above 50%. This means the model has nice performance measures, but they can be by far improved by other models. Without considering the room for improvement, the increase of performance by the use of a classification model (suitable for the proposed problem) is clearly noticeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Entropy as criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gini Index as criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Knowledge Gain as criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K-Nearest Neighbors) \"Lazy Learner\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbors algorithm is a lazy learner algorithm that classifies a new data point based on the majority class of its k-nearest neighbors. This means, that a model is not trained, but rather the algorithm searches into de training data.\n",
    "\n",
    "Decisions are made based on the majority voting for the k-nearest neighbors so they can be affected by the presence irrelevant features or attributes with different scales.\n",
    "\n",
    "Therefore, it is necessary to scale the data before applying the algorithm. We've decided to use the MinMaxScaler that scales the data but keeps its distribution, so there is no loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the features will be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the range of the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable will be encoded into a binary variable, where the quality of the wine is considered good if it is greater than or equal 6 and bad if it is less than 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = (2, 5.5, 9)\n",
    "labels= [\"poor quality\", \"nice quality\"]\n",
    "\n",
    "# Perform discretization on the quality feature\n",
    "y_discretized = pd.cut(df_wine['quality'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_discretized, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the kNN classifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN does not have a great variety of hyperparameters to tune, just the number of neighbors and the weights.\n",
    "* neighbors: the number of neighbors to consider to classify a new data point.\n",
    "* weights: the weight function used in prediction. The `uniform` means that all points in each neighborhood are weighted equally while `distance` means that the closer neighbors of a query point will have a greater influence than the neighbors that are further away.\n",
    "  \n",
    "For this reason, as the number of combinations for the hyperparameters is low and kNN is a fast algorithm, we will use the `GridSearchCV` to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 498 candidates, totalling 2490 fits\n",
      "Best Parameters from Grid Search: {'n_neighbors': 62, 'weights': 'distance'}\n",
      "Best Score from Grid Search: 0.7884985200362967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elena\\GitHub\\MachineLearning-24-25-ProgressTasks\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# Possible parameters\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 250),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "}\n",
    "# Initialize Grid Search\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, scoring='f1_macro',\n",
    "                           cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the Grid Search model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"Best Score from Grid Search:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best kNN model has the following hyperparameters:\n",
    "* n_neighbors: 62\n",
    "* weights: distance\n",
    "  \n",
    "Now, we can make the predictions on the test set and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "nice quality       0.84      0.89      0.86       849\n",
      "poor quality       0.77      0.68      0.72       451\n",
      "\n",
      "    accuracy                           0.82      1300\n",
      "   macro avg       0.80      0.78      0.79      1300\n",
      "weighted avg       0.81      0.82      0.81      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the best estimator for predictions\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_knn.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metrics shows that in general the model has an 82% of accuracy and a 0.79 as a general f1-score. This means that the model is able to classify the quality of the wine with a good accuracy. However, taking a look at the statistics for each label, we can see that the model is better at classifying the quality of the wine `nice quality` but both of them have a good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: The kNN model might be a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final decision\n",
    "\n",
    "Here we mention the best model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
