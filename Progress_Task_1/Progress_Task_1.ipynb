{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Task 1: Prediction of wine quality\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This progress task has the aim to predict the quality of wine based on its physicochemical properties. The dataset used in this task is the [Wine Quality Dataset](https://archive.ics.uci.edu/dataset/186/wine+quality) from the UCI Machine Learning Repository. Credits to *P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.*\n",
    "\n",
    "The objective of this task is to select an apropiate regression and classification model and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environmental variables\n",
    "\n",
    "Download the dataset and import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ucimlrepo seaborn matplotlib scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    " \n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    " \n",
    "# data (as pandas dataframes) \n",
    "X = wine_quality.data.features \n",
    "y = wine_quality.data.targets \n",
    "\n",
    "df_wine = pd.concat([X,y], axis=1)\n",
    " \n",
    "# metadata \n",
    "print(wine_quality.metadata) \n",
    "\n",
    "# get variable information \n",
    "print(wine_quality.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, a brief exploratory data analysis (EDA) will be performed on the dataset, prior to correctly pre-process it and capture the most relevant features for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of instances and the number of features\n",
    "print (\"Shape of data:\", X.shape , y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first rows of the features\n",
    "print(\"=================== Feature's First Rows ===================\\n\", X.head(3), \"\\n\")\n",
    "\n",
    "# Print the first rows of the target\n",
    "print(\"=================== Target's First Rows ===================\\n\", y.head(3), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=================== Null value count ===================\\n\",df_wine.isnull().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken a look into the dataset, here's a summary:\n",
    "\n",
    "The dataset consists of 11 continuous features, none of them with missing values. The features are: \n",
    "\n",
    "* `fixed_acidity`: with values ranging from 4.6 to 15.9.\n",
    "* `volatile_acidity`: with values ranging from 0.12 to 1.58.\n",
    "* `citric_acid`: with values ranging from 0 to 1.66.\n",
    "* `residual_sugar`: with values ranging from 0.6 to 65.8.\n",
    "* `chlorides`: with values ranging from 0.009 to 0.611.\n",
    "* `free_sulfur_dioxide`: with values ranging from 1 to 289.\n",
    "* `total_sulfur_dioxide`: with values ranging from 6 to 440.\n",
    "* `density`: with values ranging from 0.99 to 1.003.\n",
    "* `pH`: with values ranging from 2.74 to 4.01.\n",
    "* `sulphates`: with values ranging from 0.33 to 2.\n",
    "* `alcohol`: with values ranging from 8.4 to 14.9.\n",
    "\n",
    "The target variable is:\n",
    "* `quality`: is an integer variable, from 0 to 10 but in this dataset it ranges from 3 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the statistical summary of the dataset has been obtained, a pairplot will be created to visualize the relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='quality', data=df_wine)\n",
    "plt.title(\"Distribution of Wine Quality Ratings\")\n",
    "plt.xlabel(\"Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the plot, the target variable `quality` is not a balanced set. The majority of the wines have a quality of 5 or 6, with a few wines having a quality of 3 or 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_wine, y_vars='quality',x_vars=df_wine.columns[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplot, a strange data distribution can be observed. All the instances seem to be grouped by a certain value of the variable `quality`. The reason for this is that the target variable is **discrete**, so **it is treated as a categorical variable**.\n",
    "\n",
    "Given that no direct relation with the target can be inferred from the pairplot, the next step is to create pairplots between every pair of features. Then, the dependencies between the features will be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplot there are some interesting observations: \n",
    "\n",
    "- Fixed acidity and density seem to have a linear relationship. \n",
    "- Density seems to have a horizontal line pattern with other features, that could represent a constant value.\n",
    "\n",
    "From there, valueable information cannot be extracted, so it is necessary to continue analyzing the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corrmat = X.corr()\n",
    "sns.heatmap(corrmat, square = True, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out from the plot, the strongest correlation can be observed between the attributes **`free_sulfur_dioxide`** and **`total_sulfur_dioxide`** (0.72). The reason for this is total sulfur dioxide includes the free sulfur dioxide, so the variable free sulfur will be removed from the dataset, as both variables represent almost the same information and this will reduce redundancy.\n",
    "\n",
    "The second strongest correlation is between **`density`** and **`alcohol`** (-0.69). This correlation is negative, due to the fact that an increase in the alcohol graduation in wine leads to a loss of water quantity. Therefore, given that alcohol is less dense than water, the density of the wine decreases.\n",
    "\n",
    "maybe test to remove density as it might be a constant value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.drop(columns=['free_sulfur_dioxide'], inplace=True)\n",
    "X.drop(columns=['free_sulfur_dioxide'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of regression models\n",
    "\n",
    "We will put here the different model evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "Simple linear regression assumes the dependency of Y on X (or $X_1$, $X_2$, ... , $X_n$) is linear. In simple linear regression, we have a single predictor X. Mathematically, we can write this linear relationship as: $Y = \\beta_0 + \\beta_1X + \\epsilon$.\n",
    "\n",
    "Let's plot again the pairplot with all the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_wine, y_vars='quality',x_vars=df_wine.columns[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a line cannot be drawn to represent the relationship between the features and the target variable. This is because the target variable is discrete. However, let's try to fit a simple linear regression model for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "y = df_wine['quality']\n",
    "for i, feature in enumerate(df_wine.columns[:-1]):\n",
    "    plt.subplot(4, 3, i + 1)\n",
    "    linear = LinearRegression()\n",
    "    X = df_wine[[feature]]  # Reshape to 2D array\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    linear.fit(X_train, y_train)\n",
    "    y_pred = linear.predict(X_test)\n",
    "    plt.scatter(X_test, y_test)\n",
    "    plt.plot(X_test, y_pred, color='red')\n",
    "    plt.title(f\"{feature} {r2_score(y_test, y_pred)}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows, what we expected. That a simple linear regression model cannot be used to predict the quality of the wine that is a discrete variable.\n",
    "\n",
    "The highest $R^2$ score is 0.18, for the feature alcohol, which is very low. The rest are close to 0.\n",
    "\n",
    "**Conclusion**: Can't use simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilinear Regression\n",
    "\n",
    "#### Multilinear Regression - Ridge criterion\n",
    "\n",
    "The following block of code will make the preparations for a multilinear regression model using the Ridge criterion. The model will be trained and evaluated using the dataset. Firstly, the train-test division will be performed, then the model will be trained and evaluated following a cross validation factor of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the Train-Test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train data shape: \", X_train.shape, y_train.shape)\n",
    "\n",
    "print(\"Test data shape: \", X_test.shape, y_test.shape)\n",
    "\n",
    "# Create the Ridge Multilinear Regression model. We will\n",
    "\n",
    "ridge_regressor = RidgeCV(cv=5)\n",
    "\n",
    "# Fit the model\n",
    "ridge_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model has been successfully trained. Now, some metrics will be extracted from it:\n",
    "As we can see, a line cannot be drawn to represent the relationship between the features and the target variable. This is because the target variable is discrete. However, let's try to fit a simple linear regression model for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best lambda (alpha) selected by cross-validation\n",
    "best_lambda = ridge_regressor.alpha_\n",
    "print(f\"Best lambda selected by RidgeCV: {best_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is tested on the test set and is evaluated by the following metrics:\n",
    "\n",
    "- Mean Squares Error (MAE)\n",
    "- R² score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = ridge_regressor.predict(X_train)\n",
    "y_test_pred = ridge_regressor.predict(X_test)\n",
    "\n",
    "# Calculate both Mean Squared Error and R2 Score\n",
    "train_mse_ridge = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse_ridge = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "train_r2_ridge = r2_score(y_train, y_train_pred)\n",
    "test_r2_ridge = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train MSE: {train_mse_ridge}\")\n",
    "print(f\"Test MSE: {test_mse_ridge}\")\n",
    "print(f\"Train R2: {train_r2_ridge}\")\n",
    "print(f\"Test R2: {test_r2_ridge}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The model has not a good performance, the MSE is high and the R² score is low. This means that the model is not able to accurately predict the quality of the wine based on the physicochemical properties. This may be due to the fact that the target variable is discrete and not continuous, so a classification approach may be more suitable for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilienar Regression - Lasso criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Additive Models (GAMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final decision\n",
    "\n",
    "Here we mention the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of classification models\n",
    "\n",
    "We will put here the different model evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Entropy as criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gini Index as criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Knowledge Gain as criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier needs to be trained on categorical or discrete data, which is the case of the target variable `quality`. Therefore, the target variable will be transformed into a categorical variable with the following bins:\n",
    "* 3-5: poor quality\n",
    "* 6-9: nice quality\n",
    "\n",
    "We choose this binning because the target variable is not balanced at all. The mayority of the wines have a quality of 5 or 6, so we have tried to split it in two categories, to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins and category labels\n",
    "bins = (2, 5.5, 9)\n",
    "labels= [\"poor quality\", \"nice quality\"]\n",
    "\n",
    "y_disc = pd.cut(df_wine['quality'], bins=bins, labels=labels)\n",
    "X_disc = df_wine.drop(columns=['quality'])\n",
    "\n",
    "df_wine_discretized = pd.concat([X_disc, y_disc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='quality', data=df_wine, hue = y_disc)\n",
    "plt.title(\"Distribution of Wine Quality Ratings\")\n",
    "plt.xlabel(\"Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add text as legend\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height() +15), ha='center', va='baseline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine_discretized['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the transformation, the categories are split as follows:\n",
    "* Nice quality: 4113 instances\n",
    "* Poor quality: 2384 instances\n",
    "\n",
    "The classes are still not balanced, but the difference between qualities is not as big as before.\n",
    "\n",
    "Random Forest is robust to unbalanced classes, as it is based on majority voting. Also, to train it, we will use the parameter `class_weight='balanced'` so the weights of the classes are inversely proportional to the class frequencies. This will help to balance the data and to penalize more the errors in the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_disc, y_disc, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution for Random Search\n",
    "param_dist_random = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 5)\n",
    "}\n",
    "\n",
    "# Initialize Random Search\n",
    "# n_iter, which controls the number of different combinations to be tested\n",
    "random_search = RandomizedSearchCV(estimator=rfc, param_distributions=param_dist_random,\n",
    "                                   n_iter=100, cv=5, n_jobs=-1, verbose=2,\n",
    "                                   random_state=42, scoring='f1_weighted')\n",
    "\n",
    "# Fit the Random Search model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters from Random Search:\", random_search.best_params_)\n",
    "print(\"Best Score from Random Search:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = random_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "f1_score = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics are calculated using the parameter `average='weighted'` to take into account the class imbalance.\n",
    "The accuracy shows a value of 0.82, which means 82% of the predictions are correct. The F1 score is 0.82, which is the metric for imbalance classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow, there's the confusion matrix that shows how the model is performing. The diagonal values are the correct predictions.\n",
    "\n",
    "We can see that the model is good at predicting the 'nice quality' values, but not so good at predicting the 'poor quality' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=random_search.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Confusion Matrix for Random Forest Classifier\\nAccuracy: {accuracy:.2f}\")\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The Random Forest Classifier is a good model for this dataset. It has a good accuracy and F1 score, and it is robust to unbalanced classes. It shows a good performance on both of them, although it is better at predicting the 'nice quality' values (the `quality` values from 3 to 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K-Nearest Neighbors) \"Lazy Learner\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final decision\n",
    "\n",
    "Here we mention the best model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
