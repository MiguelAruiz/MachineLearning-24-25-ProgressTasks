{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2024/2025 - Progress Task 2 (Application of K-Nearest Neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Given that there were a few alternatives to Random Forest that could potentially outperform it, the team decided to try the algorithm that was very close in performance to it: KNN. After making some runs with the random state, KNN did not give better results than Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "K-Nearest Neighbours (KNN) is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition. This algorithm classifies itself among the simplest of all machine learning algorithms.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The team used the K-Nearest Neighbours algorithm to classify the data. The team used the following hyperparameters:\n",
    "\n",
    "- n_neighbors: 5\n",
    "- weights: distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The team used the following code to implement the K-Nearest Neighbours algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install mlflow pandas scipy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import mlflow\n",
    "import mlflow.data\n",
    "import mlflow.data.pandas_dataset\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.data.pandas_dataset import PandasDataset\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from create_dataset import Dataset\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import logging\n",
    "import configparser\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "\n",
    "# Load configuration fileconfig[\"data\"][\"targets\"]\n",
    "CONFIG_FILE_PATH = \"mlflow/test.conf\"\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_FILE_PATH)\n",
    "DATASET_PATH = config[\"data\"][\"dataset_path\"]\n",
    "TEST_DATASET_PATH = config[\"data\"][\"dataset_test_path\"]\n",
    "DATASET_INDEX_FEATURE = config[\"data\"][\"dataset_index\"]\n",
    "DATASET_TARGET_FEATURES = [\"h1n1_vaccine\", \"seasonal_vaccine\"]\n",
    "CONFIG_SECTION_MLFLOW = \"mlflow\"\n",
    "CONFIG_SECTION_NAMES = \"names\"\n",
    "CONFIG_SECTION_DATA = \"data\"\n",
    "CONFIG_PARAM_MLFLOW_ADDRESS = \"mflow_address\"\n",
    "CONFIG_PARAM_MFLWOW_PORT = \"mlflow_port\"\n",
    "CONFIG_PARAM_EXPERIMENT_NAME = \"mlflow_experiment_name\"\n",
    "MLFLOW_LOCATION = config[CONFIG_SECTION_MLFLOW][CONFIG_PARAM_MLFLOW_ADDRESS] + config[CONFIG_SECTION_MLFLOW][CONFIG_PARAM_MFLWOW_PORT]\n",
    "OPTIMIZED_SUFFIX = config[CONFIG_SECTION_NAMES][\"model_optimized\"]\n",
    "ROC_AUC_NAME = config[CONFIG_SECTION_NAMES][\"parameter_roc_auc\"]\n",
    "ACCURACY_NAME = config[CONFIG_SECTION_NAMES][\"parameter_accuracy\"]\n",
    "OUTPUT_FILE_PATH = config[CONFIG_SECTION_DATA][\"output_path\"]\n",
    "\n",
    "# Load logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S' \n",
    ")\n",
    "tuner_logger = logging.Logger(\"[Tuner]\")\n",
    "run_logger = logging.Logger(\"[Run]\")\n",
    "logger_main = logging.Logger(\"[Main]\",level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class creation\n",
    "\n",
    "class Dataset:\n",
    "    '''\n",
    "    ## Dataset\n",
    "    \n",
    "    This class represents a dataset. It handles dataset loading and splitting.\n",
    "    \n",
    "    ### Attributes\n",
    "    \n",
    "    - test: The test dataset.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor for the Dataset class.\n",
    "        '''\n",
    "        data = pd.read_csv(DATASET_PATH)\n",
    "        target = DATASET_TARGET_FEATURES\n",
    "        data.set_index(DATASET_INDEX_FEATURE, inplace=True)\n",
    "        self._y = data[target]\n",
    "        self._X = data.drop(columns=target)\n",
    "        test_data =  pd.read_csv(TEST_DATASET_PATH)\n",
    "        test_data.set_index(DATASET_INDEX_FEATURE, inplace=True)\n",
    "        self.test = test_data\n",
    "    \n",
    "    def with_correlation(self):\n",
    "        '''\n",
    "        ## with_correlation\n",
    "        Method that returns a copy of the dataset features and targets.\n",
    "        \n",
    "        ### Returns\n",
    "        (X, y): A tuple containing the dataset features and targets.\n",
    "        '''\n",
    "        \n",
    "        return self._X.copy(), self._y.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Definition for running the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This method only works for RandomForestClassifier in Grid Search. Maybe it should work with any model\n",
    "def hyperparameters(model_to_train):\n",
    "    '''\n",
    "    ## hyperparameters\n",
    "    \n",
    "    Initialize the hyperparameters for a Random Forest model and creates it.\n",
    "    \n",
    "    :param model_to_train: The model to train.\n",
    "    \n",
    "    :return model: The model, now built with fine-tuned hyperparameters.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    \n",
    "    param_dist_random = {\n",
    "                'estimator__n_estimators': randint(50, 200),\n",
    "                'estimator__max_depth': [None, 10, 20, 30],\n",
    "                'estimator__min_samples_split': randint(2, 11),\n",
    "                'estimator__min_samples_leaf': randint(1, 5)\n",
    "    }\n",
    "    tuner_logger.info(\"Hyperparameters optimized. Building model...\")\n",
    "    model = RandomizedSearchCV(estimator=model_to_train, param_distributions=param_dist_random,\n",
    "                                    n_iter=50, cv=5, n_jobs=-1, verbose=0,\n",
    "                                    scoring= ROC_AUC_NAME)\n",
    "    tuner_logger.info(\"Model built successfully!\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_model(model, model_name : str, X : pd.DataFrame, y : pd.DataFrame, output : pd.DataFrame):\n",
    "    \n",
    "    '''\n",
    "    ## play_model\n",
    "    \n",
    "    performs a Machine Learning run using the model passed as parameter and some input data.\n",
    "    \n",
    "    :param Any model: The model to train.\n",
    "    :param str model_name: The name of the model.\n",
    "    :param pd.DataFrame X: The input data.\n",
    "    :param pd.DataFrame y: The target data.\n",
    "    :param pd.DataFrame output: The data used to test the model.\n",
    "    \n",
    "    :return None: This method saves the predictions for the testing in a file within the `mlflow` directory.\n",
    "    '''\n",
    "        \n",
    "    # Split the dataset into train and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    # All this codeblock englobes the part of the run tracked by MLflow (Anything outside this block won't be tracked)\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        ################################## Initial logs ##################################\n",
    "        \n",
    "        run_logger.info(f\"========== Starting initial MLflow logging for model {model_name} =========\")\n",
    "        \n",
    "        # Log the \"presentation card\" of the model. What is it trying to achieve?\n",
    "        mlflow.set_tag(\"Objective\", \"Compare multiple models with dataset with correlation\")\n",
    "        \n",
    "        # Log the input data of the run: features and targets used for training the model.\n",
    "        pd_train = pd.concat([X_train, y_train], axis=1)\n",
    "        pd_dataset = mlflow.data.pandas_dataset.from_pandas(pd_train, \n",
    "                                                            source = \"df_encoded.csv\", name=\"whole dataset and correlation\")\n",
    "        mlflow.log_input(pd_dataset, \"training\")\n",
    "               \n",
    "        # Tune the hyperparameters of the model (if needed. Only for optimized models) and log them\n",
    "        if model_name.endswith(OPTIMIZED_SUFFIX):\n",
    "            run_logger.info(f\"Model {model_name} is optimized. Tuning hyperparameters...\")\n",
    "            model = hyperparameters(model)\n",
    "        mlflow.log_params(model.get_params())           \n",
    "\n",
    "\n",
    "        ########################## Training, testing and evaluation ######################\n",
    "        \n",
    "        # Train the model\n",
    "        run_logger.info(f\"Training model {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        signature = infer_signature(X_train, model.predict(X_train))    # Log the signature to MLFlow      \n",
    "        run_logger.info(f\"Model {model_name} trained successfully!\")\n",
    "       \n",
    "        # Predict the test data\n",
    "        run_logger.info(f\"Predicting test data with trained model {model_name}...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        run_logger.info(f\"Test data predictions finished!\")\n",
    "\n",
    "        # Evaluate the model (get the metrics)\n",
    "        run_logger.info(f\"Evaluating model {model_name}...\")\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred, average=\"macro\")\n",
    "        run_logger.info(f\"Model {model_name} evaluated successfully!\\nAccuracy: {accuracy}\\nROC AUC: {roc_auc}\")\n",
    "\n",
    "\n",
    "        ################################## Result logs ##################################\n",
    "        \n",
    "        # Log the model's metrics and information to MLflow\n",
    "        mlflow.log_metric(ROC_AUC_NAME, float(roc_auc))\n",
    "        mlflow.log_metric(ACCURACY_NAME, float(accuracy))\n",
    "        model_info = mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            signature=signature,\n",
    "            input_example=X_train,\n",
    "            registered_model_name=model_name,\n",
    "        )\n",
    "        \n",
    "        run_logger.info(f\"Model {model_name} logged successfully on MLflow.\") # I infer the model_info was for this...\n",
    "        \n",
    "        # Predict probabilities for the output data\n",
    "        predictions = model.predict_proba(output)\n",
    "        \n",
    "        h1n1_probs = predictions[0][:, 1]  # Probabilidades de clase positiva para h1n1_vaccine\n",
    "        seasonal_probs = predictions[1][:, 1]  # Probabilidades de clase positiva para seasonal_vaccine\n",
    "\n",
    "        predict = pd.DataFrame({\n",
    "            \"respondent_id\": output.index,\n",
    "            \"h1n1_vaccine\": h1n1_probs,\n",
    "            \"seasonal_vaccine\": seasonal_probs\n",
    "        })\n",
    "        \n",
    "        # The predictions are indexed by their value of respondent_id\n",
    "        predict.set_index(\"respondent_id\", inplace=True)\n",
    "        \n",
    "        ################################## Final logs ##################################\n",
    "        \n",
    "        # Store the predictions in a file and log them to MLflow\n",
    "        predict.to_csv(OUTPUT_FILE_PATH) \n",
    "        mlflow.log_artifact(OUTPUT_FILE_PATH)\n",
    "        run_logger.info(\"predictions saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Set our tracking server uri for logging\n",
    "    mlflow.set_tracking_uri(uri=MLFLOW_LOCATION)\n",
    "    experiment_name = CONFIG_PARAM_EXPERIMENT_NAME\n",
    "    if not mlflow.get_experiment_by_name(experiment_name):\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "    # Create a new MLflow Experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    logger_main.info(\"fetching data\")\n",
    "    data = Dataset()\n",
    "    X, y = data.with_correlation()\n",
    "    output = data.test\n",
    "    print(f\"Showing the number of null values for the training data:\\n {X.isnull().sum()}\")\n",
    "    print(f\"Showing the number of null values for the test data:\\n {output.isnull().sum()}\")\n",
    "    # Split the data\n",
    "    # models = {'RandomForest_no_opt': MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42), n_jobs=-1), \n",
    "    #           'RandomForest_si_opt': MultiOutputClassifier(RandomForestClassifier(random_state=42), n_jobs=-1)}\n",
    "    models = {\n",
    "        f\"{config[CONFIG_SECTION_NAMES]['knn_model_name']}\": MultiOutputClassifier(KNeighborsClassifier(n_neighbors=5, weights='distance'), n_jobs=-1),\n",
    "        }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        logger_main.info(f\"Starting run with {model_name}\")\n",
    "        play_model(model, model_name, X, y, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "On average, the results of the K-Nearest Neighbours algorithm did not outperform other previously executed algorithms, obtaining the following results:\n",
    "\n",
    "| Metric | Value |\n",
    "| --- | --- |\n",
    "| Accuracy | 0.6142|\n",
    "| ROC AUC | 0.6928 |\n",
    "\n",
    "These results have been taking from the logs of MLflow on this experiment. These results can be obseved in the following image:\n",
    "\n",
    "![KNNResults](KNNResults.png)\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This algorithm did not outperform the Random Forest algorithm, which was the best algorithm tested so far with a ROC AUC of 0.75. The team will continue to test other algorithms to find the best one for the dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
