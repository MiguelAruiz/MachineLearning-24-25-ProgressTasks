{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4484c2; text-align: center;\">Progress Task 2 (Machine Learning Model Applications and Analysis)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a summary of the work done in this second task. We will describe briefly the preprocessing steps, the models used and their performance in the competition and the lessons we have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #0096FF; text-align: center;\">FINAL POSITION 478th \n",
    "0.8606 score</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score was obtained with Catboost and hyperparameter search with Optuna. \n",
    "\n",
    "Boosting algorithms like catboost provided some great performance metrics that let us ascend to position 597. Nevertheless, the amount time and effort put on fine tuning the experiment itself was equally great for the such low results obtained by the team, we expected way more. As a common thought, we all agree that if the dataset had been processed in a better manner, applying outlier and noise reduction techniques, the performance of the model would have been much different (despite the models performing better with the dataset without modifications. But maybe this is caused by low-quality preprocessing).\n",
    "\n",
    "Here we will show the parameters of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1N1 model parameters:\n",
    "h1n1_best_model = {\n",
    "  \"best_params\": {\n",
    "    \"bagging_temperature\": 0.06253092805091326,\n",
    "    \"border_count\": 245,\n",
    "    \"depth\": 5,\n",
    "    \"grow_policy\": \"Depthwise\",\n",
    "    \"iterations\": 618,\n",
    "    \"l2_leaf_reg\": 0.5332701431047029,\n",
    "    \"learning_rate\": 0.047452186432079796,\n",
    "    \"loss_function\": \"CrossEntropy\",\n",
    "    \"random_strength\": 1.3505613330374917\n",
    "  },\n",
    "  \"best_auc_value_training\": 0.8741162463651414\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal model parameters:\n",
    "seasonal_best_model = {\n",
    "  \"best_params\": {\n",
    "    \"bagging_temperature\": 0.5075887680877953,\n",
    "    \"border_count\": 187,\n",
    "    \"depth\": 8,\n",
    "    \"grow_policy\": \"Depthwise\",\n",
    "    \"iterations\": 683,\n",
    "    \"l2_leaf_reg\": 9.962309526689461,\n",
    "    \"learning_rate\": 0.0207311303509529,\n",
    "    \"loss_function\": \"CrossEntropy\",\n",
    "    \"random_strength\": 2.1124386275626184\n",
    "  },\n",
    "  \"best_auc_value_training\": 0.8686267313659173\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had more time, more iterations of the hyperparameter search could be done to improve the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4484c2;\">Methodology</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4a44c2;\">**First attempt.** Random Forest 0.8520</div>\n",
    "\n",
    "The first thing we did was inspecting the dataset and we realize that it had too many null values. Then, we decided to delete the rows that had these values since the dataset was so large that deleting a few rows wouldn't affect the training.  \n",
    "\n",
    "**We were wrong.**\n",
    "\n",
    "When we tried to do the same with the test dataset, we realized that we couldn‚Äôt simply ignore the null values and that we needed to handle them properly. For the treatment of nulls in categorical variables, we assigned a new category called 'missing,' and for numerical values, we used -1.\n",
    "Later we removed some variables that seemed useless employment_industry or hhs_geo and encode both the training and test set using OrdinalEncoder. This is how we obtained the first dataset. After that, we needed to choose a model. \n",
    "\n",
    "Based on the results from the first task, we decided to try Random Forest with hyperparameter search, as it was the best among all the classifiers. \n",
    "\n",
    "**Result on competition: 0.8520** around 800th place which was around the middle of the leaderboard. Not bad for being the first attempt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4a44c2;\">**Trying to improve our ranking.** Gradient Boosting 0.8558</div>\n",
    "\n",
    "To improve our ranking, we decided to follow two approaches: one would explore more Random Forest, and the other would investigate other models.\n",
    "\n",
    "**1. Other models**\n",
    "\n",
    "One teammate tried MLPClassifer as an inital approach to Neural Networks while other implemented a complete pipeline using Keras. The third one, tried KNN, but it didn't improve on the Random Forest results. In addition, KNN is influenced by class imbalance and outliers, and we haven't addressed these issues in the dataset.\n",
    "\n",
    "After some time, we thought about trying other ensemble models. Searching into scikit-learn, we found out about Gradient Boosting models.\n",
    "\n",
    "**Result on competition: 0.8487 for MLPClassifier**  \n",
    "**Result on competition: 0.8464 for Neural Networks**, we had some issues with this model.  \n",
    "**Result on competition: NOT tried for KNN**  \n",
    "->**Result on competition: 0.8558 for Gradient Boosting**  \n",
    "\n",
    "\n",
    "**2. Random Forest**\n",
    "\n",
    "We already had a satisfactory result with Random Forest, but we wondered if it could be improved. Since we decided not to fix any random states, we were able to run the tests multiple times and obtain very different results, which we saved in MLflow.\n",
    "\n",
    "We tried several approaches:\n",
    "\n",
    "- Running two separate Random Forest models, one for each target, and optimizing them independently instead of using a multi-output model.\n",
    "- We also tried filtering the dataset for each target, using only the features relevant to that target. However, this approach performed much worse, as we lost relationships between the features.\n",
    "- Finally, seeing that all of this was insufficient, we decided to use a \"retraining\" method on our initial approach. This improved performance, but only slightly.\n",
    "\n",
    "We will no longer work with Random Forest as we have run out of ideas.\n",
    "\n",
    "**Result on competition: 0.8538** with the best local Random Forest. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4a44c2;\">**What's next?: Use AI.** CatBoost 0.8590</div>\n",
    "\n",
    "We thought we had reached the limit with our approach, but to our surprise, the other teams were making significant improvements. We realized that we could also enhance our ranking by using all the tools available to us: Chat GPT and Claude.\n",
    "\n",
    "We decided to try some prompt engineering by explaining to Chat GPT the context of our project, the problem we were tackling, and the models we had already tested. We asked for suggestions on other models we could use to improve our ranking score. After \"debating\" a bit with the chat, we came to the following conclusions:\n",
    "\n",
    "- We could modify the dataset: This included addressing class imbalance or eliminating correlations.  \n",
    "- Use basic models: Such as logistic regression or decision trees. However, we completely ruled this out since it wasn't aligned with our goals.  \n",
    "- **Try more complex models:** Like Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost).  \n",
    "- Optimize hyperparameters: By using libraries like Optuna or GridSearchCV.\n",
    "\n",
    "Extracts of the conversations:\n",
    "\n",
    "1. Is Optuna the same as Random Grid Search in sklearn?\n",
    "\n",
    "ü§ñ: Not exactly. Although Optuna and Grid/RandomizedSearchCV share the idea of searching for optimal hyperparameters, Optuna has several key differences that make it more flexible and efficient.\n",
    "\n",
    "2. Explain briefly XGBoost, LightGBM, CatBoost\n",
    "\n",
    "ü§ñ: All of them are not native sklearn.   \n",
    "**XGBoost**: Highly optimized for speed and performance but more complex to configure and tune.  \n",
    "**LightGBM**: Faster than XGBoost in many cases, especially for large and high-dimensional datasets, handles categorical data natively (no encoding required), however can be sensitive to imbalanced data.   \n",
    "**CatBoost**: Handles categorical features directly, often reduces overfitting automatically but has slower initial training compared to LightGBM.\n",
    "\n",
    "3. How does Catboost encode categorical variables? Is it worth we manually encode the dataset?  \n",
    "\n",
    "ü§ñ: No, it's better to use Catboost, let me explain why. Catboost natively uses a technique called 'target-based adaptative encoding' to deal with categorical variables internally. This means that while training, Catboost calculates statistics based on the target variable to encode the features. These statistics are calculated online to prevent leakage information.\n",
    "\n",
    "*Note*: these are not an exact reproduction of the conversations.\n",
    "\n",
    "**Therefore we decided to implement CatBoost with Optuna.**\n",
    "\n",
    "**Result on competition: 0.8590**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4484c2;\">Implementation</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó**NOTE**  \n",
    "The following section includes the implementation of the models. However, it‚Äôs important to note that this notebook was not used to develop the models, and we will not provide a detailed explanation of any of them here. The purpose of this notebook is to act as a summary and to compare the models. For more information, we recommend taking a look at the notebook dedicated to each specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create a report of every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a table to store the results of the models \n",
    "metrics =['Accuracy', 'F1-Macro', 'ROC AUC']\n",
    "classification_results = pd.DataFrame(columns=metrics)\n",
    "\n",
    "#Classification Summary Function\n",
    "def report_classification(y_test, y_pred, model_name, y_pred_proba):\n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    roc_auc = roc_auc_score(y_test, y_pred, average=\"macro\")\n",
    "    classification_results.loc[model_name] = [a, f1, roc_auc]\n",
    "    \n",
    "    print(\"============= METRICS FOR MODEL : \", model_name, \" =================\")\n",
    "    print(f\"Accuracy: {a}\\nF1: {f1}\\nRoc_Auc: {roc_auc}\")\n",
    "    print(\"==================== CONFUSION MATRIX ==========================\")\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    # Confusion matrix for h1n1 vaccine\n",
    "    cm_h1n1 = confusion_matrix(y_test['h1n1_vaccine'], y_pred[:, 0], normalize='all')\n",
    "    disp_h1n1 = ConfusionMatrixDisplay(confusion_matrix=cm_h1n1, display_labels=['No', 'Yes'])\n",
    "    disp_h1n1.plot(ax=ax[0], colorbar=False)\n",
    "    ax[0].set_title('Confusion Matrix for h1n1 Vaccine')\n",
    "\n",
    "    # Confusion matrix for seasonal vaccine\n",
    "    cm_seasonal = confusion_matrix(y_test['seasonal_vaccine'], y_pred[:, 1], normalize='all')\n",
    "    disp_seasonal = ConfusionMatrixDisplay(confusion_matrix=cm_seasonal, display_labels=['No', 'Yes'])\n",
    "    disp_seasonal.plot(ax=ax[1], colorbar=False)\n",
    "    ax[1].set_title('Confusion Matrix for Seasonal Vaccine')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"==================== ROC AUC Curve ==========================\")\n",
    "\n",
    "    # Get predicted probabilities for positive classes\n",
    "    y_scores = [p[:, 1] for p in y_pred_proba]\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for i, y_score in enumerate(y_scores):\n",
    "        fpr, tpr, _ = roc_curve(y_test.iloc[:, i], y_score)\n",
    "        auc = roc_auc_score(y_test.iloc[:, i], y_score)\n",
    "        plt.plot(fpr, tpr, label=f'{y_test.columns[i]} (AUC = {auc:.4f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"Random\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "from mlflow.create_dataset import Dataset\n",
    "\n",
    "data = Dataset()\n",
    "X, y = data.with_correlation()\n",
    "output = data.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4a44c2;\">Random Forest</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "model_to_train = MultiOutputClassifier(RandomForestClassifier(warm_start=True), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize search hyperparameters\n",
    "param_dist_random = {\n",
    "                'estimator__n_estimators': randint(50, 200),\n",
    "                'estimator__max_depth': [None, 10, 20, 30],\n",
    "                'estimator__min_samples_split': randint(2, 11),\n",
    "                'estimator__min_samples_leaf': randint(1, 5),\n",
    "                'estimator__criterion': ['gini', 'entropy']\n",
    "}\n",
    "random_search = RandomizedSearchCV(estimator=model_to_train, param_distributions=param_dist_random,\n",
    "                                    n_iter=100, cv=5, n_jobs=-1, verbose=1,\n",
    "                                    scoring='roc_auc', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "# WARNING time: 11 min\n",
    "random_search.fit(X_train, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "print('Best parameters found by random search:', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "report_classification(y_test, y_pred, \"Random Forest\", y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4a44c2;\">KNN</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "model = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=50, weights='uniform'), n_jobs=-1)\n",
    "\n",
    "model.fit(X_train, y_train)    \n",
    "\n",
    "y_pred_knn = model.predict(X_test)\n",
    "y_pred_proba_knn = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_classification(y_test, y_pred_knn, \"KNN\", y_pred_proba_knn)\n",
    "del y_pred_knn, y_pred_proba_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4a44c2;\">MLP Classifier</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_to_train = MultiOutputClassifier(MLPClassifier(early_stopping=True, solver='adam', \n",
    "                                                     learning_rate='constant', batch_size = 64, \n",
    "                                                     max_iter=2000), \n",
    "                                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize search hyperparameters\n",
    "param_dist = {\n",
    "    'estimator__hidden_layer_sizes': [\n",
    "        (100,), (100, 50), (150,100,50), (100, 50, 25)\n",
    "    ],\n",
    "    'estimator__activation': ['tanh', 'relu', 'logistic'],\n",
    "    'estimator__learning_rate_init': [0.0001, 0.001, 0.01],\n",
    "    'estimator__alpha': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=model_to_train, \n",
    "                                   param_distributions=param_dist, n_iter=100, \n",
    "                                   cv=5, n_jobs=-1, verbose=0, scoring = 'roc_auc', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "# WARNING time: 45 minutes\n",
    "random_search.fit(X_train, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "print('Best parameters found by grid search:', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "report_classification(y_test, y_pred_knn, \"MLP Classifier\", y_pred_proba_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4a44c2;\">Neural Networks</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further explanations on its notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4a44c2;\">Gradient Boosting</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# initialize model\n",
    "model_to_train = MultiOutputClassifier(HistGradientBoostingClassifier(early_stopping=True), n_jobs=-1)\n",
    "\n",
    "# hyperparameters optimization\n",
    "param_dist_random = {\n",
    "                'estimator__max_depth': [None, 10, 25, 50, 75, 100],\n",
    "                'estimator__min_samples_leaf': randint(5, 50),\n",
    "                'estimator__learning_rate': uniform(0.001, 0.2),\n",
    "                'estimator__max_iter': randint(50, 500),\n",
    "                'estimator__l2_regularization': uniform(0.001, 0.01),\n",
    "                'estimator__max_bins': [50, 100, 200, 255],\n",
    "    }\n",
    "random_search = RandomizedSearchCV(estimator=model_to_train, param_distributions=param_dist_random,\n",
    "                                    n_iter=100, cv=5, n_jobs=-1, verbose=1,\n",
    "                                    scoring='roc_auc', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "# WARNING: time 6 min\n",
    "random_search.fit(X_train, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "print('Best parameters found by random search:', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "report_classification(y_test, y_pred_knn, \"Gradient Boosting\", y_pred_proba_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4a44c2;\">LightGBM</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further explanations on its notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4a44c2;\">CatBoost</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further explanations on its notebook\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "        eval_metric='AUC',          \n",
    "        early_stopping_rounds=5, verbose=0         \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4484c2;\">Results</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=========================== MODELS METRICS =======================================')\n",
    "plt.figure(figsize=[16,8])\n",
    "sns.heatmap(classification_results, annot=True, cmap='Blues', fmt='.4f', vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are saved on MLFlow, organized by experiments and runs. It was easier to compare the performance of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"border: 3px solid #FFFFF; padding: 10px; border-radius: 5px; background-color: #4484c2;\">Challenges faced</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Communication  \n",
    "Our main challenge was the lack of communication between the team members. Each one of us was busy during the course and in vacations we were all spending time with our families, which made it difficult to organize our work. We could have improved the repo' structure.\n",
    "\n",
    "2. Plan in advanced   \n",
    "Another problem was not having clearly defined what tests we wanted to run. Luckily, one of the teammates insisted on using MLflow, which was incredibly useful to solve this problem. However, in future competitions we should plan in advance which models and tests will be used and share a table to store the results, as all members are supposed to be executing the models.\n",
    "\n",
    "3. Dataset size  \n",
    "It was our first time working with a dataset as big as this, with 26700 instances. It was a little difficult to preprocess the dataset because of its huge dimensionality and we had to be very patient because some models took too long to run. This task had a high computational cost both in time and resources.  \n",
    "\n",
    "4. Competitiveness   \n",
    "At first, we thought everyone will be using the techniques learnt in class, however, other classmates were ranking higher (maybe using other models), so we had no choice but to step up. At the end, this is a competition but we felt like the rules weren't clear enough. This means that, we can compete with others but among us (the class) it would have been nice to know in advance which was allowed.\n",
    "\n",
    "5. Optuna   \n",
    "Despite all of that, we have discovered an extraordinary tool, Optuna. It's very useful for optimizing hyperparameter search, and it allowed us to use the laptop while the training is executing. Also, you can save the execution of the study reload it any time. It would have been very useful to know about this tool from the beginning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
